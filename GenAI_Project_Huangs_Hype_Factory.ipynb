{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5RRc7tL9byp",
        "outputId": "fa37c070-99c5-4229-da1c-e547ce5ddb0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -q langchain langchain-community langchain-nvidia-ai-endpoints gradio rich\n",
        "%pip install -q arxiv pymupdf faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mcuFqpeHASf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA API key loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set NVIDIA API key from environment variable\n",
        "# If not found in environment, this will be None\n",
        "api_key = os.environ.get(\"NVIDIA_API_KEY\")\n",
        "\n",
        "# Print a message based on whether the key was found\n",
        "if api_key:\n",
        "    print(\"NVIDIA API key loaded successfully\")\n",
        "else:\n",
        "    raise ValueError(\"NVIDIA API key not found in .env file. Please set NVIDIA_API_KEY in your environment variables.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ_IwK8_-dSY",
        "outputId": "3b008383-16a0-4c5d-e66e-f6766f511d09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 20 of 88 available models:\n",
            "1. id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=True base_model=None\n",
            "2. id='nvidia/llama-3.1-nemotron-51b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "3. id='qwen/qwen2.5-coder-32b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "4. id='google/gemma-7b' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'] supports_tools=False supports_structured_output=False base_model=None\n",
            "5. id='yentinglin/llama-3-taiwan-70b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "6. id='microsoft/kosmos-2' model_type='nv-vlm' client='ChatNVIDIA' endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2' aliases=['ai-microsoft-kosmos-2', 'playground_kosmos_2', 'kosmos_2'] supports_tools=False supports_structured_output=False base_model=None\n",
            "7. id='mistralai/mamba-codestral-7b-v0.1' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "8. id='google/gemma-2b' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-gemma-2b', 'playground_gemma_2b', 'gemma_2b'] supports_tools=False supports_structured_output=False base_model=None\n",
            "9. id='seallms/seallm-7b-v2.5' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-seallm-7b'] supports_tools=False supports_structured_output=False base_model=None\n",
            "10. id='microsoft/phi-3.5-moe-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "11. id='microsoft/phi-3-mini-4k-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'] supports_tools=False supports_structured_output=False base_model=None\n",
            "12. id='meta/llama-3.2-11b-vision-instruct' model_type='vlm' client='ChatNVIDIA' endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions' aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "13. id='mistralai/mixtral-8x22b-instruct-v0.1' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-mixtral-8x22b-instruct'] supports_tools=False supports_structured_output=False base_model=None\n",
            "14. id='microsoft/phi-3-medium-128k-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-phi-3-medium-128k-instruct'] supports_tools=False supports_structured_output=False base_model=None\n",
            "15. id='rakuten/rakutenai-7b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "16. id='meta/llama-3.2-3b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True base_model=None\n",
            "17. id='databricks/dbrx-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-dbrx-instruct'] supports_tools=False supports_structured_output=False base_model=None\n",
            "18. id='nvidia/llama3-chatqa-1.5-70b' model_type='qa' client='ChatNVIDIA' endpoint=None aliases=['ai-chatqa-1.5-70b'] supports_tools=False supports_structured_output=False base_model=None\n",
            "19. id='nvidia/llama-3.1-nemotron-70b-reward' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "20. id='writer/palmyra-med-70b' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-palmyra-med-70b'] supports_tools=False supports_structured_output=False base_model=None\n"
          ]
        }
      ],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "# Get all available models\n",
        "all_models = ChatNVIDIA.get_available_models()\n",
        "\n",
        "# Display only the first 20 models\n",
        "print(f\"First 20 of {len(all_models)} available models:\")\n",
        "for i, model in enumerate(all_models[:20]):\n",
        "    print(f\"{i+1}. {model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kUaPrwDVkb9p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document split into 83 chunks\n"
          ]
        }
      ],
      "source": [
        "###################################### Document Summarization  ##################################\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredFileLoader, ArxivLoader\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "# Define the Pydantic model for document summarization\n",
        "class DocumentSummaryBase(BaseModel):\n",
        "    \"\"\"Base class for document summarization results.\"\"\"\n",
        "    running_summary: str = Field(default=\"\", description=\"The running summary of the document. Update, don't override.\")\n",
        "    main_ideas: List[str] = Field(default_factory=list, description=\"Maximum 3 important points from the document.\")\n",
        "    loose_ends: List[str] = Field(default_factory=list, description=\"Maximum 3 open questions or areas needing clarification.\")\n",
        "\n",
        "# Set up console formatting with NVIDIA brand color\n",
        "console = Console()\n",
        "nvidia_green = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=nvidia_green)\n",
        "\n",
        "# Document Loading \n",
        "# For local files (commented out as an option)\n",
        "# loader = UnstructuredFileLoader(\"your_document.pdf\")\n",
        "# docs = loader.load()\n",
        "\n",
        "# Load document from Arxiv (GraphRAG paper)\n",
        "loader = ArxivLoader(query=\"2404.16130\")  # GraphRAG paper\n",
        "docs = loader.load()\n",
        "\n",
        "# Configure text splitter for document chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200, \n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "# Optional preprocessing (commented out)\n",
        "# def preprocess_text(text):\n",
        "#     # Remove special characters, normalize whitespace, etc.\n",
        "#     return text.replace(\"...\", \".\")\n",
        "\n",
        "# Split documents into manageable chunks\n",
        "docs_split = text_splitter.split_documents(docs)\n",
        "print(f\"Document split into {len(docs_split)} chunks\")\n",
        "\n",
        "# Create summarization prompt\n",
        "summary_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "    You are a technical document summarizer tasked with generating a running summary of a research document.\n",
        "    You are given chunks of text from a document and an existing knowledge base containing:\n",
        "    1. A running summary of the document so far\n",
        "    2. Main ideas (max 3)\n",
        "    3. Loose ends or questions (max 3)\n",
        "    \n",
        "    Your goal is to update this knowledge base with new information from the current document chunk.\n",
        "    \n",
        "    IMPORTANT INSTRUCTIONS:\n",
        "    - DO NOT lose information when updating the knowledge base\n",
        "    - Integrate new information with existing running_summary\n",
        "    - Update main_ideas and loose_ends as needed, but keep them limited to 3 items each\n",
        "    - Follow the format instructions exactly\n",
        "    - If nothing new is present in the chunk, return the existing knowledge base unchanged\n",
        "    \n",
        "    {format_instructions}\n",
        "    \"\"\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "m6zwDYqxlSkK"
      },
      "outputs": [],
      "source": [
        "def RExtract(pydantic_class, llm, prompt):\n",
        "    '''\n",
        "    Runnable Extraction module\n",
        "    Returns a knowledge dictionary populated by slot-filling extraction\n",
        "    '''\n",
        "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
        "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
        "    def preparse(string):\n",
        "        if '{' not in string: string = '{' + string\n",
        "        if '}' not in string: string = string + '}'\n",
        "        string = (string\n",
        "            .replace(\"\\\\_\", \"_\")\n",
        "            .replace(\"\\n\", \" \")\n",
        "            .replace(r\"\\]\", \"]\")\n",
        "            .replace(r\"\\[\", \"[\")\n",
        "        )\n",
        "        # print(string)  ## Good for diagnostics\n",
        "        return string\n",
        "    return instruct_merge | prompt | llm | preparse | parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "7MRbsFh2lMx5",
        "outputId": "38414c07-9a3f-4b72-e147-3249294853df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Considered 10 documents\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[], </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m''\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "latest_summary = \"\"\n",
        "\n",
        "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
        "    def summarize_docs(docs):\n",
        "        parse_chain = RunnableAssign({'info_base' : RExtract(knowledge.__class__, llm, prompt)})\n",
        "        state = {'info_base' : knowledge}\n",
        "\n",
        "        global latest_summary  ## If your loop crashes, you can check out the latest_summary\n",
        "\n",
        "        for i, doc in enumerate(docs):\n",
        "            state['input'] = doc.page_content\n",
        "            state = parse_chain.invoke(state)\n",
        "\n",
        "            assert 'info_base' in state\n",
        "            if verbose:\n",
        "                print(f\"Considered {i+1} documents\")\n",
        "                pprint(state['info_base'])\n",
        "                latest_summary = state['info_base']\n",
        "                clear_output(wait=True)\n",
        "\n",
        "        return state['info_base']\n",
        "    return RunnableLambda(summarize_docs)\n",
        "\n",
        "instruct_model = ChatNVIDIA(model=\"mistralai/mistral-7b-instruct-v0.3\").bind(max_tokens=4096)\n",
        "instruct_llm = instruct_model | StrOutputParser()\n",
        "\n",
        "## Take the first 10 document chunks and accumulate a DocumentSummaryBase\n",
        "summarizer = RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose=True)\n",
        "summary = summarizer.invoke(docs_split[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MxWRyjbREz7e",
        "outputId": "d6770fd9-31ca-4b74-a23c-dfad4897a28d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Documents\n",
            "Chunking Documents\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge </span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">sources and discrete reasoning</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral 7B</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena </span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge \u001b[0m\n",
              "\u001b[1;38;2;118;185;0msources and discrete reasoning\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Mistral 7B\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m - Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena \u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 0\n",
            " - # Chunks: 35\n",
            " - Metadata: \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-02'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Kaiser, Illia Polosukhin'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">networks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">through an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">tasks show these models to be\\nsuperior in quality while being more parallelizable and requiring </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">task, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">English-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">after training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">literature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">constituency parsing both with large and limited training data.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-08-02'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Attention Is All You Need'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz \u001b[0m\n",
              "\u001b[32mKaiser, Illia Polosukhin'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural \u001b[0m\n",
              "\u001b[32mnetworks in an encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder \u001b[0m\n",
              "\u001b[32mthrough an attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on \u001b[0m\n",
              "\u001b[32mattention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation\u001b[0m\n",
              "\u001b[32mtasks show these models to be\\nsuperior in quality while being more parallelizable and requiring \u001b[0m\n",
              "\u001b[32msignificantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation \u001b[0m\n",
              "\u001b[32mtask, improving over the existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 \u001b[0m\n",
              "\u001b[32mEnglish-to-French\\ntranslation task, our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 \u001b[0m\n",
              "\u001b[32mafter training for 3.5 days on eight GPUs, a small fraction\\nof the training costs of the best models from the \u001b[0m\n",
              "\u001b[32mliterature. We show that the\\nTransformer generalizes well to other tasks by applying it successfully to\\nEnglish \u001b[0m\n",
              "\u001b[32mconstituency parsing both with large and limited training data.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 1\n",
            " - # Chunks: 45\n",
            " - Metadata: \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2019-05-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2019-05-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional \u001b[0m\n",
              "\u001b[32mEncoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to \u001b[0m\n",
              "\u001b[32mpre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right \u001b[0m\n",
              "\u001b[32mcontext in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output \u001b[0m\n",
              "\u001b[32mlayer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language \u001b[0m\n",
              "\u001b[32minference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and \u001b[0m\n",
              "\u001b[32mempirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, \u001b[0m\n",
              "\u001b[32mincluding\\npushing the GLUE score to 80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, MultiNLI\\naccuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% \u001b[0m\n",
              "\u001b[32mabsolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 question answering\\nTest F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD \u001b[0m\n",
              "\u001b[32mv2.0 Test F1 to 83.1\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 2\n",
            " - # Chunks: 46\n",
            " - Metadata: \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-04-12'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich K체ttler, Mike Lewis, Wen-tau Yih, Tim Rockt채schel, Sebastian Riedel, Douwe Kiela'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">precisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">behind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">world knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">downstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">models which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">models where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-04-12'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001b[0m\n",
              "\u001b[32mHeinrich K체ttler, Mike Lewis, Wen-tau Yih, Tim Rockt채schel, Sebastian Riedel, Douwe Kiela'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large pre-trained language models have been shown to store factual knowledge\\nin their parameters, \u001b[0m\n",
              "\u001b[32mand achieve state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and\u001b[0m\n",
              "\u001b[32mprecisely manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags \u001b[0m\n",
              "\u001b[32mbehind task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their \u001b[0m\n",
              "\u001b[32mworld knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism \u001b[0m\n",
              "\u001b[32mto\\nexplicit non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive \u001b[0m\n",
              "\u001b[32mdownstream tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -- \u001b[0m\n",
              "\u001b[32mmodels which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG \u001b[0m\n",
              "\u001b[32mmodels where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector \u001b[0m\n",
              "\u001b[32mindex\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which \u001b[0m\n",
              "\u001b[32mconditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different \u001b[0m\n",
              "\u001b[32mpassages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set\u001b[0m\n",
              "\u001b[32mthe state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific \u001b[0m\n",
              "\u001b[32mretrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more \u001b[0m\n",
              "\u001b[32mspecific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 3\n",
            " - # Chunks: 40\n",
            " - Metadata: \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-05-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge sources and discrete reasoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Bata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Amnon Shashua, Moshe Tenenholtz'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-05-01'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'MRKL Systems: A modular, neuro-symbolic architecture that combines large language models, external \u001b[0m\n",
              "\u001b[32mknowledge sources and discrete reasoning'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ehud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit\u001b[0m\n",
              "\u001b[32mBata, Yoav Levine, Kevin Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, \u001b[0m\n",
              "\u001b[32mAmnon Shashua, Moshe Tenenholtz'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Huge language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have ushered in a new era for AI, serving as a\\ngateway to \u001b[0m\n",
              "\u001b[32mnatural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently \u001b[0m\n",
              "\u001b[32mlimited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a \u001b[0m\n",
              "\u001b[32msystems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to \u001b[0m\n",
              "\u001b[32mlinguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete \u001b[0m\n",
              "\u001b[32mknowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, \u001b[0m\n",
              "\u001b[32mKnowledge and Language \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMRKL, pronounced \"miracle\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m system,\\nsome of the technical challenges in implementing it, \u001b[0m\n",
              "\u001b[32mand Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 4\n",
            " - # Chunks: 21\n",
            " - Metadata: \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-10-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L챕lio Renard Lavaud, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth챕e Lacroix, William El Sayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">performance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention (GQA) for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">faster\\ninference, coupled with sliding window attention (SWA) to effectively handle\\nsequences of arbitrary length</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">with a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">that surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">under the Apache 2.0 license.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-10-10'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Mistral 7B'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, \u001b[0m\n",
              "\u001b[32mDiego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L챕lio Renard Lavaud, \u001b[0m\n",
              "\u001b[32mMarie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth챕e Lacroix, William El Sayed'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\\nfor superior \u001b[0m\n",
              "\u001b[32mperformance and efficiency. Mistral 7B outperforms Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in\u001b[0m\n",
              "\u001b[32mreasoning, mathematics, and\\ncode generation. Our model leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for \u001b[0m\n",
              "\u001b[32mfaster\\ninference, coupled with sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to effectively handle\\nsequences of arbitrary length\u001b[0m\n",
              "\u001b[32mwith a reduced inference cost. We also provide a\\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, \u001b[0m\n",
              "\u001b[32mthat surpasses\\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\\nmodels are released \u001b[0m\n",
              "\u001b[32munder the Apache 2.0 license.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 5\n",
            " - # Chunks: 44\n",
            " - Metadata: \n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Evaluating large language model (LLM) based chat assistants is challenging\\ndue to their broad </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">capabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">using strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">limitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">LLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Arena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">controlled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">between humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">are otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">each other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conversations with\\nhuman preferences are publicly available </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-12-24'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, \u001b[0m\n",
              "\u001b[32mZhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Evaluating large language model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m based chat assistants is challenging\\ndue to their broad \u001b[0m\n",
              "\u001b[32mcapabilities and the inadequacy of existing benchmarks in\\nmeasuring human preferences. To address this, we explore\u001b[0m\n",
              "\u001b[32musing strong LLMs as\\njudges to evaluate these models on more open-ended questions. We examine the\\nusage and \u001b[0m\n",
              "\u001b[32mlimitations of LLM-as-a-judge, including position, verbosity, and\\nself-enhancement biases, as well as limited \u001b[0m\n",
              "\u001b[32mreasoning ability, and propose\\nsolutions to mitigate some of them. We then verify the agreement between \u001b[0m\n",
              "\u001b[32mLLM\\njudges and human preferences by introducing two benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot \u001b[0m\n",
              "\u001b[32mArena, a crowdsourced battle platform. Our\\nresults reveal that strong LLM judges like GPT-4 can match both \u001b[0m\n",
              "\u001b[32mcontrolled and\\ncrowdsourced human preferences well, achieving over 80% agreement, the same\\nlevel of agreement \u001b[0m\n",
              "\u001b[32mbetween humans. Hence, LLM-as-a-judge is a scalable and\\nexplainable way to approximate human preferences, which \u001b[0m\n",
              "\u001b[32mare otherwise very\\nexpensive to obtain. Additionally, we show our benchmark and traditional\\nbenchmarks complement\u001b[0m\n",
              "\u001b[32meach other by evaluating several variants of LLaMA and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K \u001b[0m\n",
              "\u001b[32mconversations with\\nhuman preferences are publicly available \u001b[0m\n",
              "\u001b[32mat\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "###################################### Retrieval-Augmented Generation (RAG) ##################################\n",
        "\n",
        "import json\n",
        "import gradio as gr\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import ArxivLoader, UnstructuredPDFLoader\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain.document_transformers import LongContextReorder\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain.schema.runnable import RunnableLambda\n",
        "from langchain.schema.runnable.passthrough import RunnableAssign\n",
        "from faiss import IndexFlatL2\n",
        "from operator import itemgetter\n",
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "console = Console()\n",
        "base_style = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=base_style)\n",
        "\n",
        "# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "# ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
        "\n",
        "## Utility Runnables/Methods\n",
        "def RPrint(preface=\"\"):\n",
        "    \"\"\"Simple passthrough \"prints, then returns\" chain\"\"\"\n",
        "    def print_and_return(x, preface):\n",
        "        if preface: print(preface, end=\"\")\n",
        "        pprint(x)\n",
        "        return x\n",
        "    return RunnableLambda(partial(print_and_return, preface=preface))\n",
        "\n",
        "def docs2str(docs, title=\"Document\"):\n",
        "    \"\"\"Useful utility for making chunks into context string. Optional, but useful\"\"\"\n",
        "    out_str = \"\"\n",
        "    for doc in docs:\n",
        "        doc_name = getattr(doc, 'metadata', {}).get('Title', title)\n",
        "        if doc_name:\n",
        "            out_str += f\"[Quote from {doc_name}] \"\n",
        "        out_str += getattr(doc, 'page_content', str(doc)) + \"\\n\"\n",
        "    return out_str\n",
        "\n",
        "## Optional; Reorders longer documents to center of output text\n",
        "long_reorder = RunnableLambda(LongContextReorder().transform_documents)\n",
        "\n",
        "## Data Loading/Text Splitting\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"],\n",
        ")\n",
        "\n",
        "print(\"Loading Documents\")\n",
        "docs = [\n",
        "    ArxivLoader(query=\"1706.03762\").load(),  ## Attention Is All You Need Paper\n",
        "    ArxivLoader(query=\"1810.04805\").load(),  ## BERT Paper\n",
        "    ArxivLoader(query=\"2005.11401\").load(),  ## RAG Paper\n",
        "    ArxivLoader(query=\"2205.00445\").load(),  ## MRKL Paper\n",
        "    ArxivLoader(query=\"2310.06825\").load(),  ## Mistral Paper\n",
        "    ArxivLoader(query=\"2306.05685\").load(),  ## LLM-as-a-Judge\n",
        "    ## Some longer papers\n",
        "    # ArxivLoader(query=\"2210.03629\").load(),  ## ReAct Paper\n",
        "    # ArxivLoader(query=\"2112.10752\").load(),  ## Latent Stable Diffusion Paper\n",
        "    # ArxivLoader(query=\"2103.00020\").load(),  ## CLIP Paper\n",
        "]\n",
        "\n",
        "## Cut the paper short if references is included.\n",
        "## This is a standard string in papers.\n",
        "for doc in docs:\n",
        "    content = json.dumps(doc[0].page_content)\n",
        "    if \"References\" in content:\n",
        "        doc[0].page_content = content[:content.index(\"References\")]\n",
        "\n",
        "## Split the documents and also filter out stubs (overly short chunks)\n",
        "print(\"Chunking Documents\")\n",
        "docs_chunks = [text_splitter.split_documents(doc) for doc in docs]\n",
        "docs_chunks = [[c for c in dchunks if len(c.page_content) > 200] for dchunks in docs_chunks]\n",
        "\n",
        "## Make some custom Chunks to give big-picture details\n",
        "doc_string = \"Available Documents:\"\n",
        "doc_metadata = []\n",
        "for chunks in docs_chunks:\n",
        "    metadata = getattr(chunks[0], 'metadata', {})\n",
        "    doc_string += \"\\n - \" + metadata.get('Title')\n",
        "    doc_metadata += [str(metadata)]\n",
        "\n",
        "extra_chunks = [doc_string] + doc_metadata\n",
        "\n",
        "## Printing out some summary information for reference\n",
        "pprint(doc_string, '\\n')\n",
        "for i, chunks in enumerate(docs_chunks):\n",
        "    print(f\"Document {i}\")\n",
        "    print(f\" - # Chunks: {len(chunks)}\")\n",
        "    print(f\" - Metadata: \")\n",
        "    pprint(chunks[0].metadata)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz2AOChME7J_",
        "outputId": "54262dea-d13e-48fe-966a-cb54326217f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Constructing Vector Stores\n",
            "Constructed aggregate docstore with 238 chunks\n",
            "CPU times: user 557 ms, sys: 30.6 ms, total: 588 ms\n",
            "Wall time: 16.6 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "## Constructing Your Document Vector Stores\n",
        "print(\"Constructing Vector Stores\")\n",
        "vecstores = [FAISS.from_texts(extra_chunks, embedder)]\n",
        "vecstores += [FAISS.from_documents(doc_chunks, embedder) for doc_chunks in docs_chunks]\n",
        "\n",
        "embed_dims = len(embedder.embed_query(\"test\"))\n",
        "def default_FAISS():\n",
        "    '''Useful utility for making an empty FAISS vectorstore'''\n",
        "    return FAISS(\n",
        "        embedding_function=embedder,\n",
        "        index=IndexFlatL2(embed_dims),\n",
        "        docstore=InMemoryDocstore(),\n",
        "        index_to_docstore_id={},\n",
        "        normalize_L2=False\n",
        "    )\n",
        "\n",
        "def aggregate_vstores(vectorstores):\n",
        "    ## Initialize an empty FAISS Index and merge others into it\n",
        "    ## We'll use default_faiss for simplicity, though it's tied to your embedder by reference\n",
        "    agg_vstore = default_FAISS()\n",
        "    for vstore in vectorstores:\n",
        "        agg_vstore.merge_from(vstore)\n",
        "    return agg_vstore\n",
        "\n",
        "## Unintuitive optimization; merge_from seems to optimize constituent vector stores away\n",
        "docstore = aggregate_vstores(vecstores)\n",
        "\n",
        "print(f\"Constructed aggregate docstore with {len(docstore.docstore._dict)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_ZGftNJDFd3d",
        "outputId": "9ed73807-4500-48f8-ae3a-98e3d38371cd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'history'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . We refer to this </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">decoding procedure as \\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">requiring many forward passes. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">p\\\\u03b8(y|x, zi) \\\\u22480 where y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">additional forward passes once the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\u201cFast Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">tasks. For all experiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">et al. [31] and\\\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article is split into </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">disjoint\\\\n100-word chunks, to make a total of 21M documents\\n[Quote from Retrieval-Augmented Generation for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge-Intensive NLP Tasks] . Since RAG can be\\\\nemployed as a language model, similar concerns as for GPT-2 </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">[50] are valid here, although arguably\\\\nto a lesser extent, including that it might be used to generate abuse, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">faked or misleading content in\\\\nthe news or on social media; to impersonate others; or to automate the production </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">of spam/phishing\\\\ncontent [54]. Advanced language models may also lead to the automation of various jobs in </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the\\\\ncoming decades [16]. In order to mitigate these risks, AI systems could be employed to \\\\ufb01ght </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">against\\\\nmisleading content and automated spam/phishing.\\\\nAcknowledgments\\\\nThe authors would like to thank the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reviewers for their thoughtful and constructive feedback on this\\\\npaper, as well as HuggingFace for their help in </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">open-sourcing code to run RAG models. The authors\\\\nwould also like to thank Kyunghyun Cho and Sewon Min for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">productive discussions and advice. EP\\\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">by the FAIR PhD\\\\nprogram.\\\\n\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">.\\\\nRAG-Token\\\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\\\ntor with transition</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">probability: p\\\\u2032\\\\n\\\\u03b8(yi|x, y1:i\\\\u22121) = P\\\\nz\\\\u2208top-k(p(\\\\u00b7|x)) p\\\\u03b7(zi|x)p\\\\u03b8(yi|x, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">zi, y1:i\\\\u22121) To\\\\ndecode, we can plug p\\\\u2032\\\\n\\\\u03b8(yi|x, y1:i\\\\u22121) into a standard beam </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">decoder.\\\\nRAG-Sequence\\\\nFor RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-\\\\ntoken </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\\\neach document z, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">scoring each hypothesis using p\\\\u03b8(yi|x, z, y1:i\\\\u22121). This yields a set of hypotheses\\\\nY , some of which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">may not have appeared in the beams of all documents. To estimate the probability\\\\nof an hypothesis y we run an </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">additional forward pass for each document z for which y does not\\\\nappear in the beam, multiply generator </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">probability with p\\\\u03b7(z|x) and then sum the probabilities across\\\\nbeams for the marginals\\n[Quote from </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . In one approach, RAG-Sequence, the model uses </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the same document\\\\nto predict each target token. The second approach, RAG-Token, can predict each target token </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">based\\\\non a different document. In the following, we formally introduce both models and then describe </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as the training and decoding </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same retrieved document to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">that\\\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation\\n'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Tell me about RAG!'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'history'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m''\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'context'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . We refer to this \u001b[0m\n",
              "\u001b[32mdecoding procedure as \\\\u201cThorough Decoding.\\\\u201d For longer\\\\noutput sequences, |Y | can become large, \u001b[0m\n",
              "\u001b[32mrequiring many forward passes. For more ef\\\\ufb01cient decoding,\\\\nwe can make a further approximation that \u001b[0m\n",
              "\u001b[32mp\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x, zi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \\\\u22480 where y was not generated during beam\\\\nsearch from x, zi. This avoids the need to run \u001b[0m\n",
              "\u001b[32madditional forward passes once the candidate set Y has\\\\nbeen generated. We refer to this decoding procedure as \u001b[0m\n",
              "\u001b[32m\\\\u201cFast Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment with RAG in a wide range of knowledge-intensive \u001b[0m\n",
              "\u001b[32mtasks. For all experiments, we use\\\\na single Wikipedia dump for our non-parametric knowledge source. Following Lee\u001b[0m\n",
              "\u001b[32met al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m31\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and\\\\nKarpukhin et al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, we use the December 2018 dump. Each Wikipedia article is split into \u001b[0m\n",
              "\u001b[32mdisjoint\\\\n100-word chunks, to make a total of 21M documents\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for \u001b[0m\n",
              "\u001b[32mKnowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Since RAG can be\\\\nemployed as a language model, similar concerns as for GPT-2 \u001b[0m\n",
              "\u001b[32m[\u001b[0m\u001b[32m50\u001b[0m\u001b[32m]\u001b[0m\u001b[32m are valid here, although arguably\\\\nto a lesser extent, including that it might be used to generate abuse, \u001b[0m\n",
              "\u001b[32mfaked or misleading content in\\\\nthe news or on social media; to impersonate others; or to automate the production \u001b[0m\n",
              "\u001b[32mof spam/phishing\\\\ncontent \u001b[0m\u001b[32m[\u001b[0m\u001b[32m54\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Advanced language models may also lead to the automation of various jobs in \u001b[0m\n",
              "\u001b[32mthe\\\\ncoming decades \u001b[0m\u001b[32m[\u001b[0m\u001b[32m16\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In order to mitigate these risks, AI systems could be employed to \\\\ufb01ght \u001b[0m\n",
              "\u001b[32magainst\\\\nmisleading content and automated spam/phishing.\\\\nAcknowledgments\\\\nThe authors would like to thank the \u001b[0m\n",
              "\u001b[32mreviewers for their thoughtful and constructive feedback on this\\\\npaper, as well as HuggingFace for their help in \u001b[0m\n",
              "\u001b[32mopen-sourcing code to run RAG models. The authors\\\\nwould also like to thank Kyunghyun Cho and Sewon Min for \u001b[0m\n",
              "\u001b[32mproductive discussions and advice. EP\\\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported \u001b[0m\n",
              "\u001b[32mby the FAIR PhD\\\\nprogram.\\\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32m.\\\\nRAG-Token\\\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\\\ntor with transition\u001b[0m\n",
              "\u001b[32mprobability: p\\\\u2032\\\\n\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, y1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m = P\\\\nz\\\\u2208top-k\u001b[0m\u001b[32m(\u001b[0m\u001b[32mp\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\u00b7|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m p\\\\u03b7\u001b[0m\u001b[32m(\u001b[0m\u001b[32mzi|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32mp\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, \u001b[0m\n",
              "\u001b[32mzi, y1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m To\\\\ndecode, we can plug p\\\\u2032\\\\n\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, y1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m into a standard beam \u001b[0m\n",
              "\u001b[32mdecoder.\\\\nRAG-Sequence\\\\nFor RAG-Sequence, the likelihood p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m does not break into a conventional per-\\\\ntoken \u001b[0m\n",
              "\u001b[32mlikelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\\\neach document z, \u001b[0m\n",
              "\u001b[32mscoring each hypothesis using p\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, z, y1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This yields a set of hypotheses\\\\nY , some of which \u001b[0m\n",
              "\u001b[32mmay not have appeared in the beams of all documents. To estimate the probability\\\\nof an hypothesis y we run an \u001b[0m\n",
              "\u001b[32madditional forward pass for each document z for which y does not\\\\nappear in the beam, multiply generator \u001b[0m\n",
              "\u001b[32mprobability with p\\\\u03b7\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and then sum the probabilities across\\\\nbeams for the marginals\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from \u001b[0m\n",
              "\u001b[32mRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . In one approach, RAG-Sequence, the model uses \u001b[0m\n",
              "\u001b[32mthe same document\\\\nto predict each target token. The second approach, RAG-Token, can predict each target token \u001b[0m\n",
              "\u001b[32mbased\\\\non a different document. In the following, we formally introduce both models and then describe \u001b[0m\n",
              "\u001b[32mthe\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as the training and decoding \u001b[0m\n",
              "\u001b[32mprocedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same retrieved document to \u001b[0m\n",
              "\u001b[32mgenerate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable \u001b[0m\n",
              "\u001b[32mthat\\\\nis marginalized to get the seq2seq probability p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m via a top-K approximation\\n'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Conversation History Retrieval:\\n\\n\\n Document Retrieval:\\n[Quote from Retrieval-Augmented Generation for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge-Intensive NLP Tasks] . We refer to this decoding procedure as \\\\u201cThorough Decoding.\\\\u201d For </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">longer\\\\noutput sequences, |Y | can become large, requiring many forward passes. For more ef\\\\ufb01cient </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">decoding,\\\\nwe can make a further approximation that p\\\\u03b8(y|x, zi) \\\\u22480 where y was not generated during </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">beam\\\\nsearch from x, zi. This avoids the need to run additional forward passes once the candidate set Y has\\\\nbeen</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">generated. We refer to this decoding procedure as \\\\u201cFast Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">with RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\\\\na single Wikipedia dump for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">our non-parametric knowledge source. Following Lee et al. [31] and\\\\nKarpukhin et al. [26], we use the December </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2018 dump. Each Wikipedia article is split into disjoint\\\\n100-word chunks, to make a total of 21M </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">documents\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . Since RAG can </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">be\\\\nemployed as a language model, similar concerns as for GPT-2 [50] are valid here, although arguably\\\\nto a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">lesser extent, including that it might be used to generate abuse, faked or misleading content in\\\\nthe news or on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">social media; to impersonate others; or to automate the production of spam/phishing\\\\ncontent [54]. Advanced </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">language models may also lead to the automation of various jobs in the\\\\ncoming decades [16]. In order to mitigate </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">these risks, AI systems could be employed to \\\\ufb01ght against\\\\nmisleading content and automated </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">spam/phishing.\\\\nAcknowledgments\\\\nThe authors would like to thank the reviewers for their thoughtful and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">constructive feedback on this\\\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">models. The authors\\\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">EP\\\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">PhD\\\\nprogram.\\\\n\\n[Quote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">.\\\\nRAG-Token\\\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\\\ntor with transition</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">probability: p\\\\u2032\\\\n\\\\u03b8(yi|x, y1:i\\\\u22121) = P\\\\nz\\\\u2208top-k(p(\\\\u00b7|x)) p\\\\u03b7(zi|x)p\\\\u03b8(yi|x, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">zi, y1:i\\\\u22121) To\\\\ndecode, we can plug p\\\\u2032\\\\n\\\\u03b8(yi|x, y1:i\\\\u22121) into a standard beam </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">decoder.\\\\nRAG-Sequence\\\\nFor RAG-Sequence, the likelihood p(y|x) does not break into a conventional per-\\\\ntoken </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">likelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\\\neach document z, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">scoring each hypothesis using p\\\\u03b8(yi|x, z, y1:i\\\\u22121). This yields a set of hypotheses\\\\nY , some of which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">may not have appeared in the beams of all documents. To estimate the probability\\\\nof an hypothesis y we run an </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">additional forward pass for each document z for which y does not\\\\nappear in the beam, multiply generator </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">probability with p\\\\u03b7(z|x) and then sum the probabilities across\\\\nbeams for the marginals\\n[Quote from </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks] . In one approach, RAG-Sequence, the model uses </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the same document\\\\nto predict each target token. The second approach, RAG-Token, can predict each target token </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">based\\\\non a different document. In the following, we formally introduce both models and then describe </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as the training and decoding </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">procedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same retrieved document to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">generate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">that\\\\nis marginalized to get the seq2seq probability p(y|x) via a top-K approximation\\n\\n\\n (Answer only from </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">retrieval. Only cite sources that are used. Make your response conversational.)'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={})</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
              "\u001b[32mjust asked: Tell me about RAG!\\n\\n From this, we have retrieved the following potentially-useful info:  \u001b[0m\n",
              "\u001b[32mConversation History Retrieval:\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for \u001b[0m\n",
              "\u001b[32mKnowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . We refer to this decoding procedure as \\\\u201cThorough Decoding.\\\\u201d For \u001b[0m\n",
              "\u001b[32mlonger\\\\noutput sequences, |Y | can become large, requiring many forward passes. For more ef\\\\ufb01cient \u001b[0m\n",
              "\u001b[32mdecoding,\\\\nwe can make a further approximation that p\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x, zi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \\\\u22480 where y was not generated during \u001b[0m\n",
              "\u001b[32mbeam\\\\nsearch from x, zi. This avoids the need to run additional forward passes once the candidate set Y has\\\\nbeen\u001b[0m\n",
              "\u001b[32mgenerated. We refer to this decoding procedure as \\\\u201cFast Decoding.\\\\u201d\\\\n3\\\\nExperiments\\\\nWe experiment \u001b[0m\n",
              "\u001b[32mwith RAG in a wide range of knowledge-intensive tasks. For all experiments, we use\\\\na single Wikipedia dump for \u001b[0m\n",
              "\u001b[32mour non-parametric knowledge source. Following Lee et al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m31\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and\\\\nKarpukhin et al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, we use the December \u001b[0m\n",
              "\u001b[32m2018 dump. Each Wikipedia article is split into disjoint\\\\n100-word chunks, to make a total of 21M \u001b[0m\n",
              "\u001b[32mdocuments\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . Since RAG can \u001b[0m\n",
              "\u001b[32mbe\\\\nemployed as a language model, similar concerns as for GPT-2 \u001b[0m\u001b[32m[\u001b[0m\u001b[32m50\u001b[0m\u001b[32m]\u001b[0m\u001b[32m are valid here, although arguably\\\\nto a \u001b[0m\n",
              "\u001b[32mlesser extent, including that it might be used to generate abuse, faked or misleading content in\\\\nthe news or on \u001b[0m\n",
              "\u001b[32msocial media; to impersonate others; or to automate the production of spam/phishing\\\\ncontent \u001b[0m\u001b[32m[\u001b[0m\u001b[32m54\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Advanced \u001b[0m\n",
              "\u001b[32mlanguage models may also lead to the automation of various jobs in the\\\\ncoming decades \u001b[0m\u001b[32m[\u001b[0m\u001b[32m16\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. In order to mitigate \u001b[0m\n",
              "\u001b[32mthese risks, AI systems could be employed to \\\\ufb01ght against\\\\nmisleading content and automated \u001b[0m\n",
              "\u001b[32mspam/phishing.\\\\nAcknowledgments\\\\nThe authors would like to thank the reviewers for their thoughtful and \u001b[0m\n",
              "\u001b[32mconstructive feedback on this\\\\npaper, as well as HuggingFace for their help in open-sourcing code to run RAG \u001b[0m\n",
              "\u001b[32mmodels. The authors\\\\nwould also like to thank Kyunghyun Cho and Sewon Min for productive discussions and advice. \u001b[0m\n",
              "\u001b[32mEP\\\\nthanks supports from the NSF Graduate Research Fellowship. PL is supported by the FAIR \u001b[0m\n",
              "\u001b[32mPhD\\\\nprogram.\\\\n\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32m.\\\\nRAG-Token\\\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\\\ntor with transition\u001b[0m\n",
              "\u001b[32mprobability: p\\\\u2032\\\\n\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, y1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m = P\\\\nz\\\\u2208top-k\u001b[0m\u001b[32m(\u001b[0m\u001b[32mp\u001b[0m\u001b[32m(\u001b[0m\u001b[32m\\\\u00b7|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m p\\\\u03b7\u001b[0m\u001b[32m(\u001b[0m\u001b[32mzi|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32mp\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, \u001b[0m\n",
              "\u001b[32mzi, y1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m To\\\\ndecode, we can plug p\\\\u2032\\\\n\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, y1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m into a standard beam \u001b[0m\n",
              "\u001b[32mdecoder.\\\\nRAG-Sequence\\\\nFor RAG-Sequence, the likelihood p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m does not break into a conventional per-\\\\ntoken \u001b[0m\n",
              "\u001b[32mlikelihood, hence we cannot solve it with a single beam search. Instead, we run beam search for\\\\neach document z, \u001b[0m\n",
              "\u001b[32mscoring each hypothesis using p\\\\u03b8\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, z, y1:i\\\\u22121\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This yields a set of hypotheses\\\\nY , some of which \u001b[0m\n",
              "\u001b[32mmay not have appeared in the beams of all documents. To estimate the probability\\\\nof an hypothesis y we run an \u001b[0m\n",
              "\u001b[32madditional forward pass for each document z for which y does not\\\\nappear in the beam, multiply generator \u001b[0m\n",
              "\u001b[32mprobability with p\\\\u03b7\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and then sum the probabilities across\\\\nbeams for the marginals\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from \u001b[0m\n",
              "\u001b[32mRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . In one approach, RAG-Sequence, the model uses \u001b[0m\n",
              "\u001b[32mthe same document\\\\nto predict each target token. The second approach, RAG-Token, can predict each target token \u001b[0m\n",
              "\u001b[32mbased\\\\non a different document. In the following, we formally introduce both models and then describe \u001b[0m\n",
              "\u001b[32mthe\\\\np\\\\u03b7 and p\\\\u03b8 components, as well as the training and decoding \u001b[0m\n",
              "\u001b[32mprocedure.\\\\n2.1\\\\nModels\\\\nRAG-Sequence Model\\\\nThe RAG-Sequence model uses the same retrieved document to \u001b[0m\n",
              "\u001b[32mgenerate\\\\nthe complete sequence. Technically, it treats the retrieved document as a single latent variable \u001b[0m\n",
              "\u001b[32mthat\\\\nis marginalized to get the seq2seq probability p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m via a top-K approximation\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from \u001b[0m\n",
              "\u001b[32mretrieval. Only cite sources that are used. Make your response conversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Tell me about RAG!'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG stands for Retrieval-Augmented Generation. It is a method that combines a retriever model, which identifies relevant documents from a large corpus, with a generator model, which produces answers to natural language processing (NLP) tasks. The retriever model is based on the DPR (Dense Passage Retrieval) model, and the generator model is based on a transformer model.\n",
            "\n",
            "RAG can be employed in a variety of knowledge-intensive NLP tasks and is trained on a single Wikipedia dump, which is split into disjoint 100-word chunks to make a total of 21M documents.\n",
            "\n",
            "There are two ways that RAG can be used, RAG-Sequence and RAG-Token. RAG-Sequence uses the same document to predict each target token, while RAG-Token can predict each target token based on a different document.\n",
            "\n",
            "There are also two decoding procedures that can be used with RAG, Thorough Decoding and Fast Decoding. Thorough decoding runs additional forward passes for longer output sequences, while Fast Decoding makes an approximation that p(y|x,zi)  0 where y was not generated during the beam search from x, zi. This avoids the need to run additional forward passes once the candidate set Y has been generated.\n",
            "\n",
            "It's important to note that similar concerns to GPT-2 apply to RAG, including the potential for misuse in generating abuse, faked or misleading content, impersonating others and automating the production of spam/phishing content.\n",
            "Therefore, AI systems should be employed to fight against misleading content and automated spam/phishing.\n",
            "\n",
            "The RAG model was introduced in the paper \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Lewis, Linford, et.al, 2020) and the documentation provided is from that paper.\n",
            "\n",
            "\n",
            "References:\n",
            "- Lewis, Parker, et al. \"Retrieval-augmented generation for knowledge-intensive nlp tasks.\" arXiv preprint arXiv:2002.08913 (2020).\n",
            "- Radford, Alec, et al. \"Language models are unsupervised multitask learners.\" OpenAI blog 1.8 (2019): 9.\n",
            "- Karpukhin, Vsevolod, et al. \"Dense Passage Retrieval for Open-Domain Question Answering.\" arXiv preprint arXiv:2004.04906 (2020)."
          ]
        }
      ],
      "source": [
        "# NVIDIAEmbeddings.get_available_models()\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "# ChatNVIDIA.get_available_models()\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
        "# instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
        "\n",
        "convstore = default_FAISS()\n",
        "\n",
        "def save_memory_and_get_output(d, vstore):\n",
        "    \"\"\"Accepts 'input'/'output' dictionary and saves to convstore\"\"\"\n",
        "    vstore.add_texts([\n",
        "        f\"User previously responded with {d.get('input')}\",\n",
        "        f\"Agent previously responded with {d.get('output')}\"\n",
        "    ])\n",
        "    return d.get('output')\n",
        "\n",
        "initial_msg = (\n",
        "    \"Hello! I am a document chat agent here to help the user!\"\n",
        "    f\" I have access to the following documents: {doc_string}\\n\\nHow can I help you?\"\n",
        ")\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([(\"system\",\n",
        "    \"You are a document chatbot. Help the user as they ask questions about documents.\"\n",
        "    \" User messaged just asked: {input}\\n\\n\"\n",
        "    \" From this, we have retrieved the following potentially-useful info: \"\n",
        "    \" Conversation History Retrieval:\\n{history}\\n\\n\"\n",
        "    \" Document Retrieval:\\n{context}\\n\\n\"\n",
        "    \" (Answer only from retrieval. Only cite sources that are used. Make your response conversational.)\"\n",
        "), ('user', '{input}')])\n",
        "\n",
        "## Implement the Stream Chain\n",
        "stream_chain = chat_prompt| RPrint() | instruct_llm | StrOutputParser()\n",
        "\n",
        "## Implement the RAG Chain\n",
        "retrieval_chain = (\n",
        "    {'input' : (lambda x: x)}\n",
        "    | RunnableAssign({'history' : itemgetter('input') | convstore.as_retriever() | long_reorder | docs2str})\n",
        "    | RunnableAssign({'context' : itemgetter('input') | docstore.as_retriever()  | long_reorder | docs2str})\n",
        "    | RPrint()\n",
        ")\n",
        "\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "    buffer = \"\"\n",
        "    ## First perform the retrieval based on the input message\n",
        "    retrieval = retrieval_chain.invoke(message)\n",
        "    line_buffer = \"\"\n",
        "\n",
        "    ## Then, stream the results of the stream_chain\n",
        "    for token in stream_chain.stream(retrieval):\n",
        "        buffer += token\n",
        "        ## If you're using standard print, keep line from getting too long\n",
        "        yield buffer if return_buffer else token\n",
        "\n",
        "    ## Lastly, save the chat exchange to the conversation memory buffer\n",
        "    save_memory_and_get_output({'input':  message, 'output': buffer}, convstore)\n",
        "\n",
        "\n",
        "## Start of Agent Event Loop\n",
        "test_question = \"Tell me about RAG!\"  ## <- modify as desired\n",
        "\n",
        "## Before you launch your gradio interface, make sure your thing works\n",
        "for response in chat_gen(test_question, return_buffer=False):\n",
        "    print(response, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kyyeFx1mFnqN",
        "outputId": "2680959c-cdc8-4dee-c079-63e41d4e43b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-28-7a9f48738b41>:1: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:317: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://66d7f153f0437a03c7.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://66d7f153f0437a03c7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">{</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'input'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about  BERT'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'history'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'[Quote from Document] User previously responded with Tell me about RAG!\\n[Quote from Document] </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Agent previously responded with RAG stands for Retrieval-Augmented Generation. It is a method that combines a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">retriever model, which identifies relevant documents from a large corpus, with a generator model, which produces </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">answers to natural language processing (NLP) tasks. The retriever model is based on the DPR (Dense Passage </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval) model, and the generator model is based on a transformer model.\\n\\nRAG can be employed in a variety of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive NLP tasks and is trained on a single Wikipedia dump, which is split into disjoint 100-word </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">chunks to make a total of 21M documents.\\n\\nThere are two ways that RAG can be used, RAG-Sequence and RAG-Token. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">RAG-Sequence uses the same document to predict each target token, while RAG-Token can predict each target token </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">based on a different document.\\n\\nThere are also two decoding procedures that can be used with RAG, Thorough </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Decoding and Fast Decoding. Thorough decoding runs additional forward passes for longer output sequences, while </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Fast Decoding makes an approximation that p(y|x,zi)  0 where y was not generated during the beam search from x, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">zi. This avoids the need to run additional forward passes once the candidate set Y has been generated.\\n\\nIt\\'s </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">important to note that similar concerns to GPT-2 apply to RAG, including the potential for misuse in generating </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">abuse, faked or misleading content, impersonating others and automating the production of spam/phishing </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">content.\\nTherefore, AI systems should be employed to fight against misleading content and automated </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">spam/phishing.\\n\\nThe RAG model was introduced in the paper \"Retrieval-Augmented Generation for Knowledge-Intensive</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">NLP Tasks\" (Lewis, Linford, et.al, 2020) and the documentation provided is from that paper.\\n\\n\\nReferences:\\n- </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Lewis, Parker, et al. \"Retrieval-augmented generation for knowledge-intensive nlp tasks.\" arXiv preprint </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">arXiv:2002.08913 (2020).\\n- Radford, Alec, et al. \"Language models are unsupervised multitask learners.\" OpenAI </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">blog 1.8 (2019): 9.\\n- Karpukhin, Vsevolod, et al. \"Dense Passage Retrieval for Open-Domain Question Answering.\" </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">arXiv preprint arXiv:2004.04906 (2020).\\n'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #008000; text-decoration-color: #008000\">'context'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'[Quote from BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding] </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">.\\\\nBERT is conceptually simple and empirically\\\\npowerful.\\\\nIt obtains new state-of-the-art re-\\\\nsults on eleven</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">natural language processing\\\\ntasks, including pushing the GLUE score to\\\\n80.5% (7.7% point absolute </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">improvement),\\\\nMultiNLI accuracy to 86.7% (4.6% absolute\\\\nimprovement), SQuAD v1.1 question answer-\\\\ning Test F1</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to 93.2 (1.5 point absolute im-\\\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\\\n(5.1 point absolute </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">improvement).\\\\n1\\\\nIntroduction\\\\nLanguage model pre-training has been shown to\\\\nbe effective for improving many </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">natural language\\\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\\\n2018a; Radford et al., 2018; Howard and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Ruder,\\\\n2018). These include sentence-level tasks such as\\\\nnatural language inference (Bowman et al., </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2015;\\\\nWilliams et al\\n[Quote from Document] {\\'Published\\': \\'2019-05-24\\', \\'Title\\': \\'BERT: Pre-training of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Deep Bidirectional Transformers for Language Understanding\\', \\'Authors\\': \\'Jacob Devlin, Ming-Wei Chang, Kenton </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Lee, Kristina Toutanova\\', \\'Summary\\': \\'We introduce a new language representation model called BERT, which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">stands\\\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\\\nlanguage representation </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">models, BERT is designed to pre-train deep\\\\nbidirectional representations from unlabeled text by jointly </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conditioning on\\\\nboth left and right context in all layers. As a result, the pre-trained BERT\\\\nmodel can be </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuned with just one additional output layer to create\\\\nstate-of-the-art models for a wide range of tasks, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">such as question answering\\\\nand language inference, without substantial task-specific </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">architecture\\\\nmodifications.\\\\n  BERT is conceptually simple and empirically powerful. It obtains </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">new\\\\nstate-of-the-art results on eleven natural language processing tasks, including\\\\npushing the GLUE score to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">80.5% (7.7% point absolute improvement), MultiNLI\\\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">question answering\\\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\\\\n(5.1 point </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement).\\'}\\n[Quote from BERT: Pre-training of Deep Bidirectional Transformers for Language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Understanding] . The contributions\\\\nof our paper are as follows:\\\\n\\\\u2022 We demonstrate the importance of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">bidirectional\\\\npre-training for language representations. Un-\\\\nlike Radford et al. (2018), which uses </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">unidirec-\\\\ntional language models for pre-training, BERT\\\\nuses masked language models to enable pre-\\\\ntrained </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">deep bidirectional representations. This\\\\nis also in contrast to Peters et al. (2018a), which\\\\nuses a shallow </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">concatenation of independently\\\\ntrained left-to-right and right-to-left LMs.\\\\n\\\\u2022 We show that pre-trained </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">representations reduce\\\\nthe need for many heavily-engineered task-\\\\nspeci\\\\ufb01c architectures. BERT is the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\ufb01rst \\\\ufb01ne-\\\\ntuning based representation model that achieves\\\\nstate-of-the-art performance on a large </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">suite\\\\nof sentence-level and token-level tasks, outper-\\\\nforming many task-speci\\\\ufb01c architectures.\\\\n\\\\u2022</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">BERT advances the state of the art for eleven\\\\nNLP tasks.\\\\nThe code and pre-trained mod-\\\\nels are available at </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">https://github\\n[Quote from BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding] </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\"BERT: Pre-training of Deep Bidirectional Transformers for\\\\nLanguage Understanding\\\\nJacob Devlin\\\\nMing-Wei </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Chang\\\\nKenton Lee\\\\nKristina Toutanova\\\\nGoogle AI </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Language\\\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\\\nAbstract\\\\nWe introduce a new language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">representa-\\\\ntion model called BERT, which stands for\\\\nBidirectional Encoder Representations from\\\\nTransformers.</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Unlike recent language repre-\\\\nsentation models (Peters et al., 2018a; Rad-\\\\nford et al., 2018), BERT is designed</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to pre-\\\\ntrain deep bidirectional representations from\\\\nunlabeled text by jointly conditioning on both\\\\nleft and</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">right context in all layers. As a re-\\\\nsult, the pre-trained BERT model can be \\\\ufb01ne-\\\\ntuned with just one </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">additional output layer\\\\nto create state-of-the-art models for a wide\\\\nrange of tasks, such as question answering</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\nlanguage inference, without substantial task-\\\\nspeci\\\\ufb01c architecture modi\\\\ufb01cations.\\\\nBERT is </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conceptually simple and empirically\\\\npowerful\\n'</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m{\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'input'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Tell me about  BERT'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'history'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m User previously responded with Tell me about RAG!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32mAgent previously responded with RAG stands for Retrieval-Augmented Generation. It is a method that combines a \u001b[0m\n",
              "\u001b[32mretriever model, which identifies relevant documents from a large corpus, with a generator model, which produces \u001b[0m\n",
              "\u001b[32manswers to natural language processing \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNLP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m tasks. The retriever model is based on the DPR \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDense Passage \u001b[0m\n",
              "\u001b[32mRetrieval\u001b[0m\u001b[32m)\u001b[0m\u001b[32m model, and the generator model is based on a transformer model.\\n\\nRAG can be employed in a variety of \u001b[0m\n",
              "\u001b[32mknowledge-intensive NLP tasks and is trained on a single Wikipedia dump, which is split into disjoint 100-word \u001b[0m\n",
              "\u001b[32mchunks to make a total of 21M documents.\\n\\nThere are two ways that RAG can be used, RAG-Sequence and RAG-Token. \u001b[0m\n",
              "\u001b[32mRAG-Sequence uses the same document to predict each target token, while RAG-Token can predict each target token \u001b[0m\n",
              "\u001b[32mbased on a different document.\\n\\nThere are also two decoding procedures that can be used with RAG, Thorough \u001b[0m\n",
              "\u001b[32mDecoding and Fast Decoding. Thorough decoding runs additional forward passes for longer output sequences, while \u001b[0m\n",
              "\u001b[32mFast Decoding makes an approximation that p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x,zi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m  0 where y was not generated during the beam search from x, \u001b[0m\n",
              "\u001b[32mzi. This avoids the need to run additional forward passes once the candidate set Y has been generated.\\n\\nIt\\'s \u001b[0m\n",
              "\u001b[32mimportant to note that similar concerns to GPT-2 apply to RAG, including the potential for misuse in generating \u001b[0m\n",
              "\u001b[32mabuse, faked or misleading content, impersonating others and automating the production of spam/phishing \u001b[0m\n",
              "\u001b[32mcontent.\\nTherefore, AI systems should be employed to fight against misleading content and automated \u001b[0m\n",
              "\u001b[32mspam/phishing.\\n\\nThe RAG model was introduced in the paper \"Retrieval-Augmented Generation for Knowledge-Intensive\u001b[0m\n",
              "\u001b[32mNLP Tasks\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLewis, Linford, et.al, 2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and the documentation provided is from that paper.\\n\\n\\nReferences:\\n- \u001b[0m\n",
              "\u001b[32mLewis, Parker, et al. \"Retrieval-augmented generation for knowledge-intensive nlp tasks.\" arXiv preprint \u001b[0m\n",
              "\u001b[32marXiv:2002.08913 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n- Radford, Alec, et al. \"Language models are unsupervised multitask learners.\" OpenAI \u001b[0m\n",
              "\u001b[32mblog 1.8 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: 9.\\n- Karpukhin, Vsevolod, et al. \"Dense Passage Retrieval for Open-Domain Question Answering.\" \u001b[0m\n",
              "\u001b[32marXiv preprint arXiv:2004.04906 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[32m'context'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32m.\\\\nBERT is conceptually simple and empirically\\\\npowerful.\\\\nIt obtains new state-of-the-art re-\\\\nsults on eleven\u001b[0m\n",
              "\u001b[32mnatural language processing\\\\ntasks, including pushing the GLUE score to\\\\n80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute \u001b[0m\n",
              "\u001b[32mimprovement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\\\nMultiNLI accuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% absolute\\\\nimprovement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 question answer-\\\\ning Test F1\u001b[0m\n",
              "\u001b[32mto 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute im-\\\\nprovement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD v2.0 Test F1 to 83.1\\\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute \u001b[0m\n",
              "\u001b[32mimprovement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\\\n1\\\\nIntroduction\\\\nLanguage model pre-training has been shown to\\\\nbe effective for improving many \u001b[0m\n",
              "\u001b[32mnatural language\\\\nprocessing tasks \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDai and Le, 2015; Peters et al.,\\\\n2018a; Radford et al., 2018; Howard and \u001b[0m\n",
              "\u001b[32mRuder,\\\\n2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. These include sentence-level tasks such as\\\\nnatural language inference \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBowman et al., \u001b[0m\n",
              "\u001b[32m2015;\\\\nWilliams et al\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'Published\\': \\'2019-05-24\\', \\'Title\\': \\'BERT: Pre-training of \u001b[0m\n",
              "\u001b[32mDeep Bidirectional Transformers for Language Understanding\\', \\'Authors\\': \\'Jacob Devlin, Ming-Wei Chang, Kenton \u001b[0m\n",
              "\u001b[32mLee, Kristina Toutanova\\', \\'Summary\\': \\'We introduce a new language representation model called BERT, which \u001b[0m\n",
              "\u001b[32mstands\\\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\\\nlanguage representation \u001b[0m\n",
              "\u001b[32mmodels, BERT is designed to pre-train deep\\\\nbidirectional representations from unlabeled text by jointly \u001b[0m\n",
              "\u001b[32mconditioning on\\\\nboth left and right context in all layers. As a result, the pre-trained BERT\\\\nmodel can be \u001b[0m\n",
              "\u001b[32mfine-tuned with just one additional output layer to create\\\\nstate-of-the-art models for a wide range of tasks, \u001b[0m\n",
              "\u001b[32msuch as question answering\\\\nand language inference, without substantial task-specific \u001b[0m\n",
              "\u001b[32marchitecture\\\\nmodifications.\\\\n  BERT is conceptually simple and empirically powerful. It obtains \u001b[0m\n",
              "\u001b[32mnew\\\\nstate-of-the-art results on eleven natural language processing tasks, including\\\\npushing the GLUE score to \u001b[0m\n",
              "\u001b[32m80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, MultiNLI\\\\naccuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 \u001b[0m\n",
              "\u001b[32mquestion answering\\\\nTest F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD v2.0 Test F1 to 83.1\\\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point \u001b[0m\n",
              "\u001b[32mabsolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from BERT: Pre-training of Deep Bidirectional Transformers for Language \u001b[0m\n",
              "\u001b[32mUnderstanding\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . The contributions\\\\nof our paper are as follows:\\\\n\\\\u2022 We demonstrate the importance of \u001b[0m\n",
              "\u001b[32mbidirectional\\\\npre-training for language representations. Un-\\\\nlike Radford et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which uses \u001b[0m\n",
              "\u001b[32munidirec-\\\\ntional language models for pre-training, BERT\\\\nuses masked language models to enable pre-\\\\ntrained \u001b[0m\n",
              "\u001b[32mdeep bidirectional representations. This\\\\nis also in contrast to Peters et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2018a\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which\\\\nuses a shallow \u001b[0m\n",
              "\u001b[32mconcatenation of independently\\\\ntrained left-to-right and right-to-left LMs.\\\\n\\\\u2022 We show that pre-trained \u001b[0m\n",
              "\u001b[32mrepresentations reduce\\\\nthe need for many heavily-engineered task-\\\\nspeci\\\\ufb01c architectures. BERT is the \u001b[0m\n",
              "\u001b[32m\\\\ufb01rst \\\\ufb01ne-\\\\ntuning based representation model that achieves\\\\nstate-of-the-art performance on a large \u001b[0m\n",
              "\u001b[32msuite\\\\nof sentence-level and token-level tasks, outper-\\\\nforming many task-speci\\\\ufb01c architectures.\\\\n\\\\u2022\u001b[0m\n",
              "\u001b[32mBERT advances the state of the art for eleven\\\\nNLP tasks.\\\\nThe code and pre-trained mod-\\\\nels are available at \u001b[0m\n",
              "\u001b[32mhttps://github\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32m\"BERT: Pre-training of Deep Bidirectional Transformers for\\\\nLanguage Understanding\\\\nJacob Devlin\\\\nMing-Wei \u001b[0m\n",
              "\u001b[32mChang\\\\nKenton Lee\\\\nKristina Toutanova\\\\nGoogle AI \u001b[0m\n",
              "\u001b[32mLanguage\\\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mjacobdevlin,mingweichang,kentonl,kristout\u001b[0m\u001b[32m}\u001b[0m\u001b[32m@google.com\\\\nAbstract\\\\nWe introduce a new language \u001b[0m\n",
              "\u001b[32mrepresenta-\\\\ntion model called BERT, which stands for\\\\nBidirectional Encoder Representations from\\\\nTransformers.\u001b[0m\n",
              "\u001b[32mUnlike recent language repre-\\\\nsentation models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPeters et al., 2018a; Rad-\\\\nford et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, BERT is designed\u001b[0m\n",
              "\u001b[32mto pre-\\\\ntrain deep bidirectional representations from\\\\nunlabeled text by jointly conditioning on both\\\\nleft and\u001b[0m\n",
              "\u001b[32mright context in all layers. As a re-\\\\nsult, the pre-trained BERT model can be \\\\ufb01ne-\\\\ntuned with just one \u001b[0m\n",
              "\u001b[32madditional output layer\\\\nto create state-of-the-art models for a wide\\\\nrange of tasks, such as question answering\u001b[0m\n",
              "\u001b[32mand\\\\nlanguage inference, without substantial task-\\\\nspeci\\\\ufb01c architecture modi\\\\ufb01cations.\\\\nBERT is \u001b[0m\n",
              "\u001b[32mconceptually simple and empirically\\\\npowerful\\n'\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ChatPromptValue</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'You are a document chatbot. Help the user as they ask questions about documents. User messaged</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">just asked: Tell me about  BERT\\n\\n From this, we have retrieved the following potentially-useful info:  </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Conversation History Retrieval:\\n[Quote from Document] User previously responded with Tell me about RAG!\\n[Quote </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">from Document] Agent previously responded with RAG stands for Retrieval-Augmented Generation. It is a method that </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">combines a retriever model, which identifies relevant documents from a large corpus, with a generator model, which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">produces answers to natural language processing (NLP) tasks. The retriever model is based on the DPR (Dense Passage</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Retrieval) model, and the generator model is based on a transformer model.\\n\\nRAG can be employed in a variety of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive NLP tasks and is trained on a single Wikipedia dump, which is split into disjoint 100-word </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">chunks to make a total of 21M documents.\\n\\nThere are two ways that RAG can be used, RAG-Sequence and RAG-Token. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">RAG-Sequence uses the same document to predict each target token, while RAG-Token can predict each target token </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">based on a different document.\\n\\nThere are also two decoding procedures that can be used with RAG, Thorough </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Decoding and Fast Decoding. Thorough decoding runs additional forward passes for longer output sequences, while </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Fast Decoding makes an approximation that p(y|x,zi)  0 where y was not generated during the beam search from x, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">zi. This avoids the need to run additional forward passes once the candidate set Y has been generated.\\n\\nIt\\'s </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">important to note that similar concerns to GPT-2 apply to RAG, including the potential for misuse in generating </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">abuse, faked or misleading content, impersonating others and automating the production of spam/phishing </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">content.\\nTherefore, AI systems should be employed to fight against misleading content and automated </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">spam/phishing.\\n\\nThe RAG model was introduced in the paper \"Retrieval-Augmented Generation for Knowledge-Intensive</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">NLP Tasks\" (Lewis, Linford, et.al, 2020) and the documentation provided is from that paper.\\n\\n\\nReferences:\\n- </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Lewis, Parker, et al. \"Retrieval-augmented generation for knowledge-intensive nlp tasks.\" arXiv preprint </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">arXiv:2002.08913 (2020).\\n- Radford, Alec, et al. \"Language models are unsupervised multitask learners.\" OpenAI </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">blog 1.8 (2019): 9.\\n- Karpukhin, Vsevolod, et al. \"Dense Passage Retrieval for Open-Domain Question Answering.\" </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">arXiv preprint arXiv:2004.04906 (2020).\\n\\n\\n Document Retrieval:\\n[Quote from BERT: Pre-training of Deep </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Bidirectional Transformers for Language Understanding] .\\\\nBERT is conceptually simple and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">empirically\\\\npowerful.\\\\nIt obtains new state-of-the-art re-\\\\nsults on eleven natural language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">processing\\\\ntasks, including pushing the GLUE score to\\\\n80.5% (7.7% point absolute improvement),\\\\nMultiNLI </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">accuracy to 86.7% (4.6% absolute\\\\nimprovement), SQuAD v1.1 question answer-\\\\ning Test F1 to 93.2 (1.5 point </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">absolute im-\\\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\\\n(5.1 point absolute </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">improvement).\\\\n1\\\\nIntroduction\\\\nLanguage model pre-training has been shown to\\\\nbe effective for improving many </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">natural language\\\\nprocessing tasks (Dai and Le, 2015; Peters et al.,\\\\n2018a; Radford et al., 2018; Howard and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Ruder,\\\\n2018). These include sentence-level tasks such as\\\\nnatural language inference (Bowman et al., </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2015;\\\\nWilliams et al\\n[Quote from Document] {\\'Published\\': \\'2019-05-24\\', \\'Title\\': \\'BERT: Pre-training of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Deep Bidirectional Transformers for Language Understanding\\', \\'Authors\\': \\'Jacob Devlin, Ming-Wei Chang, Kenton </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Lee, Kristina Toutanova\\', \\'Summary\\': \\'We introduce a new language representation model called BERT, which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">stands\\\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\\\nlanguage representation </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">models, BERT is designed to pre-train deep\\\\nbidirectional representations from unlabeled text by jointly </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conditioning on\\\\nboth left and right context in all layers. As a result, the pre-trained BERT\\\\nmodel can be </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuned with just one additional output layer to create\\\\nstate-of-the-art models for a wide range of tasks, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">such as question answering\\\\nand language inference, without substantial task-specific </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">architecture\\\\nmodifications.\\\\n  BERT is conceptually simple and empirically powerful. It obtains </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">new\\\\nstate-of-the-art results on eleven natural language processing tasks, including\\\\npushing the GLUE score to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">80.5% (7.7% point absolute improvement), MultiNLI\\\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">question answering\\\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\\\\n(5.1 point </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">absolute improvement).\\'}\\n[Quote from BERT: Pre-training of Deep Bidirectional Transformers for Language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Understanding] . The contributions\\\\nof our paper are as follows:\\\\n\\\\u2022 We demonstrate the importance of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">bidirectional\\\\npre-training for language representations. Un-\\\\nlike Radford et al. (2018), which uses </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">unidirec-\\\\ntional language models for pre-training, BERT\\\\nuses masked language models to enable pre-\\\\ntrained </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">deep bidirectional representations. This\\\\nis also in contrast to Peters et al. (2018a), which\\\\nuses a shallow </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">concatenation of independently\\\\ntrained left-to-right and right-to-left LMs.\\\\n\\\\u2022 We show that pre-trained </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">representations reduce\\\\nthe need for many heavily-engineered task-\\\\nspeci\\\\ufb01c architectures. BERT is the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\\\\ufb01rst \\\\ufb01ne-\\\\ntuning based representation model that achieves\\\\nstate-of-the-art performance on a large </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">suite\\\\nof sentence-level and token-level tasks, outper-\\\\nforming many task-speci\\\\ufb01c architectures.\\\\n\\\\u2022</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">BERT advances the state of the art for eleven\\\\nNLP tasks.\\\\nThe code and pre-trained mod-\\\\nels are available at </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">https://github\\n[Quote from BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding] </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\"BERT: Pre-training of Deep Bidirectional Transformers for\\\\nLanguage Understanding\\\\nJacob Devlin\\\\nMing-Wei </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Chang\\\\nKenton Lee\\\\nKristina Toutanova\\\\nGoogle AI </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Language\\\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\\\nAbstract\\\\nWe introduce a new language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">representa-\\\\ntion model called BERT, which stands for\\\\nBidirectional Encoder Representations from\\\\nTransformers.</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Unlike recent language repre-\\\\nsentation models (Peters et al., 2018a; Rad-\\\\nford et al., 2018), BERT is designed</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to pre-\\\\ntrain deep bidirectional representations from\\\\nunlabeled text by jointly conditioning on both\\\\nleft and</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">right context in all layers. As a re-\\\\nsult, the pre-trained BERT model can be \\\\ufb01ne-\\\\ntuned with just one </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">additional output layer\\\\nto create state-of-the-art models for a wide\\\\nrange of tasks, such as question answering</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and\\\\nlanguage inference, without substantial task-\\\\nspeci\\\\ufb01c architecture modi\\\\ufb01cations.\\\\nBERT is </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conceptually simple and empirically\\\\npowerful\\n\\n\\n (Answer only from retrieval. Only cite sources that are used. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Make your response conversational.)'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">,</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={},</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">            </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        ),</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">        </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about  BERT'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={})</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">    ]</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">)</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mChatPromptValue\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'You are a document chatbot. Help the user as they ask questions about documents. User messaged\u001b[0m\n",
              "\u001b[32mjust asked: Tell me about  BERT\\n\\n From this, we have retrieved the following potentially-useful info:  \u001b[0m\n",
              "\u001b[32mConversation History Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m User previously responded with Tell me about RAG!\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote \u001b[0m\n",
              "\u001b[32mfrom Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m Agent previously responded with RAG stands for Retrieval-Augmented Generation. It is a method that \u001b[0m\n",
              "\u001b[32mcombines a retriever model, which identifies relevant documents from a large corpus, with a generator model, which \u001b[0m\n",
              "\u001b[32mproduces answers to natural language processing \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNLP\u001b[0m\u001b[32m)\u001b[0m\u001b[32m tasks. The retriever model is based on the DPR \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDense Passage\u001b[0m\n",
              "\u001b[32mRetrieval\u001b[0m\u001b[32m)\u001b[0m\u001b[32m model, and the generator model is based on a transformer model.\\n\\nRAG can be employed in a variety of \u001b[0m\n",
              "\u001b[32mknowledge-intensive NLP tasks and is trained on a single Wikipedia dump, which is split into disjoint 100-word \u001b[0m\n",
              "\u001b[32mchunks to make a total of 21M documents.\\n\\nThere are two ways that RAG can be used, RAG-Sequence and RAG-Token. \u001b[0m\n",
              "\u001b[32mRAG-Sequence uses the same document to predict each target token, while RAG-Token can predict each target token \u001b[0m\n",
              "\u001b[32mbased on a different document.\\n\\nThere are also two decoding procedures that can be used with RAG, Thorough \u001b[0m\n",
              "\u001b[32mDecoding and Fast Decoding. Thorough decoding runs additional forward passes for longer output sequences, while \u001b[0m\n",
              "\u001b[32mFast Decoding makes an approximation that p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x,zi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m  0 where y was not generated during the beam search from x, \u001b[0m\n",
              "\u001b[32mzi. This avoids the need to run additional forward passes once the candidate set Y has been generated.\\n\\nIt\\'s \u001b[0m\n",
              "\u001b[32mimportant to note that similar concerns to GPT-2 apply to RAG, including the potential for misuse in generating \u001b[0m\n",
              "\u001b[32mabuse, faked or misleading content, impersonating others and automating the production of spam/phishing \u001b[0m\n",
              "\u001b[32mcontent.\\nTherefore, AI systems should be employed to fight against misleading content and automated \u001b[0m\n",
              "\u001b[32mspam/phishing.\\n\\nThe RAG model was introduced in the paper \"Retrieval-Augmented Generation for Knowledge-Intensive\u001b[0m\n",
              "\u001b[32mNLP Tasks\" \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLewis, Linford, et.al, 2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and the documentation provided is from that paper.\\n\\n\\nReferences:\\n- \u001b[0m\n",
              "\u001b[32mLewis, Parker, et al. \"Retrieval-augmented generation for knowledge-intensive nlp tasks.\" arXiv preprint \u001b[0m\n",
              "\u001b[32marXiv:2002.08913 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n- Radford, Alec, et al. \"Language models are unsupervised multitask learners.\" OpenAI \u001b[0m\n",
              "\u001b[32mblog 1.8 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2019\u001b[0m\u001b[32m)\u001b[0m\u001b[32m: 9.\\n- Karpukhin, Vsevolod, et al. \"Dense Passage Retrieval for Open-Domain Question Answering.\" \u001b[0m\n",
              "\u001b[32marXiv preprint arXiv:2004.04906 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2020\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n\\n Document Retrieval:\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from BERT: Pre-training of Deep \u001b[0m\n",
              "\u001b[32mBidirectional Transformers for Language Understanding\u001b[0m\u001b[32m]\u001b[0m\u001b[32m .\\\\nBERT is conceptually simple and \u001b[0m\n",
              "\u001b[32mempirically\\\\npowerful.\\\\nIt obtains new state-of-the-art re-\\\\nsults on eleven natural language \u001b[0m\n",
              "\u001b[32mprocessing\\\\ntasks, including pushing the GLUE score to\\\\n80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\\\nMultiNLI \u001b[0m\n",
              "\u001b[32maccuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% absolute\\\\nimprovement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 question answer-\\\\ning Test F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point \u001b[0m\n",
              "\u001b[32mabsolute im-\\\\nprovement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD v2.0 Test F1 to 83.1\\\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute \u001b[0m\n",
              "\u001b[32mimprovement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\\\n1\\\\nIntroduction\\\\nLanguage model pre-training has been shown to\\\\nbe effective for improving many \u001b[0m\n",
              "\u001b[32mnatural language\\\\nprocessing tasks \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDai and Le, 2015; Peters et al.,\\\\n2018a; Radford et al., 2018; Howard and \u001b[0m\n",
              "\u001b[32mRuder,\\\\n2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. These include sentence-level tasks such as\\\\nnatural language inference \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBowman et al., \u001b[0m\n",
              "\u001b[32m2015;\\\\nWilliams et al\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from Document\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\u001b[32m{\u001b[0m\u001b[32m\\'Published\\': \\'2019-05-24\\', \\'Title\\': \\'BERT: Pre-training of \u001b[0m\n",
              "\u001b[32mDeep Bidirectional Transformers for Language Understanding\\', \\'Authors\\': \\'Jacob Devlin, Ming-Wei Chang, Kenton \u001b[0m\n",
              "\u001b[32mLee, Kristina Toutanova\\', \\'Summary\\': \\'We introduce a new language representation model called BERT, which \u001b[0m\n",
              "\u001b[32mstands\\\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\\\nlanguage representation \u001b[0m\n",
              "\u001b[32mmodels, BERT is designed to pre-train deep\\\\nbidirectional representations from unlabeled text by jointly \u001b[0m\n",
              "\u001b[32mconditioning on\\\\nboth left and right context in all layers. As a result, the pre-trained BERT\\\\nmodel can be \u001b[0m\n",
              "\u001b[32mfine-tuned with just one additional output layer to create\\\\nstate-of-the-art models for a wide range of tasks, \u001b[0m\n",
              "\u001b[32msuch as question answering\\\\nand language inference, without substantial task-specific \u001b[0m\n",
              "\u001b[32marchitecture\\\\nmodifications.\\\\n  BERT is conceptually simple and empirically powerful. It obtains \u001b[0m\n",
              "\u001b[32mnew\\\\nstate-of-the-art results on eleven natural language processing tasks, including\\\\npushing the GLUE score to \u001b[0m\n",
              "\u001b[32m80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, MultiNLI\\\\naccuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 \u001b[0m\n",
              "\u001b[32mquestion answering\\\\nTest F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD v2.0 Test F1 to 83.1\\\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point \u001b[0m\n",
              "\u001b[32mabsolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\'\u001b[0m\u001b[32m}\u001b[0m\u001b[32m\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from BERT: Pre-training of Deep Bidirectional Transformers for Language \u001b[0m\n",
              "\u001b[32mUnderstanding\u001b[0m\u001b[32m]\u001b[0m\u001b[32m . The contributions\\\\nof our paper are as follows:\\\\n\\\\u2022 We demonstrate the importance of \u001b[0m\n",
              "\u001b[32mbidirectional\\\\npre-training for language representations. Un-\\\\nlike Radford et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which uses \u001b[0m\n",
              "\u001b[32munidirec-\\\\ntional language models for pre-training, BERT\\\\nuses masked language models to enable pre-\\\\ntrained \u001b[0m\n",
              "\u001b[32mdeep bidirectional representations. This\\\\nis also in contrast to Peters et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2018a\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which\\\\nuses a shallow \u001b[0m\n",
              "\u001b[32mconcatenation of independently\\\\ntrained left-to-right and right-to-left LMs.\\\\n\\\\u2022 We show that pre-trained \u001b[0m\n",
              "\u001b[32mrepresentations reduce\\\\nthe need for many heavily-engineered task-\\\\nspeci\\\\ufb01c architectures. BERT is the \u001b[0m\n",
              "\u001b[32m\\\\ufb01rst \\\\ufb01ne-\\\\ntuning based representation model that achieves\\\\nstate-of-the-art performance on a large \u001b[0m\n",
              "\u001b[32msuite\\\\nof sentence-level and token-level tasks, outper-\\\\nforming many task-speci\\\\ufb01c architectures.\\\\n\\\\u2022\u001b[0m\n",
              "\u001b[32mBERT advances the state of the art for eleven\\\\nNLP tasks.\\\\nThe code and pre-trained mod-\\\\nels are available at \u001b[0m\n",
              "\u001b[32mhttps://github\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mQuote from BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u001b[0m\u001b[32m]\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32m\"BERT: Pre-training of Deep Bidirectional Transformers for\\\\nLanguage Understanding\\\\nJacob Devlin\\\\nMing-Wei \u001b[0m\n",
              "\u001b[32mChang\\\\nKenton Lee\\\\nKristina Toutanova\\\\nGoogle AI \u001b[0m\n",
              "\u001b[32mLanguage\\\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mjacobdevlin,mingweichang,kentonl,kristout\u001b[0m\u001b[32m}\u001b[0m\u001b[32m@google.com\\\\nAbstract\\\\nWe introduce a new language \u001b[0m\n",
              "\u001b[32mrepresenta-\\\\ntion model called BERT, which stands for\\\\nBidirectional Encoder Representations from\\\\nTransformers.\u001b[0m\n",
              "\u001b[32mUnlike recent language repre-\\\\nsentation models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPeters et al., 2018a; Rad-\\\\nford et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, BERT is designed\u001b[0m\n",
              "\u001b[32mto pre-\\\\ntrain deep bidirectional representations from\\\\nunlabeled text by jointly conditioning on both\\\\nleft and\u001b[0m\n",
              "\u001b[32mright context in all layers. As a re-\\\\nsult, the pre-trained BERT model can be \\\\ufb01ne-\\\\ntuned with just one \u001b[0m\n",
              "\u001b[32madditional output layer\\\\nto create state-of-the-art models for a wide\\\\nrange of tasks, such as question answering\u001b[0m\n",
              "\u001b[32mand\\\\nlanguage inference, without substantial task-\\\\nspeci\\\\ufb01c architecture modi\\\\ufb01cations.\\\\nBERT is \u001b[0m\n",
              "\u001b[32mconceptually simple and empirically\\\\npowerful\\n\\n\\n \u001b[0m\u001b[32m(\u001b[0m\u001b[32mAnswer only from retrieval. Only cite sources that are used. \u001b[0m\n",
              "\u001b[32mMake your response conversational.\u001b[0m\u001b[32m)\u001b[0m\u001b[32m'\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m            \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m,\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m        \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Tell me about  BERT'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m    \u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://66d7f153f0437a03c7.gradio.live\n",
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
        "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
        "\n",
        "try:\n",
        "    demo.launch(debug=True, share=True, show_api=False)\n",
        "    demo.close()\n",
        "except Exception as e:\n",
        "    demo.close()\n",
        "    print(e)\n",
        "    raise e"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
