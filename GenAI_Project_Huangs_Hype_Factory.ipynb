{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t5RRc7tL9byp",
        "outputId": "fa37c070-99c5-4229-da1c-e547ce5ddb0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "### CODE BLOCK 1\n",
        "%pip install -q langchain langchain-community langchain-nvidia-ai-endpoints gradio rich\n",
        "%pip install -q arxiv pymupdf faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcuFqpeHASf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA API key loaded successfully\n"
          ]
        }
      ],
      "source": [
        "### CODE BLOCK 2\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()\n",
        "\n",
        "# Set NVIDIA API key from environment variable\n",
        "# If not found in environment, this will be None\n",
        "api_key = os.environ.get(\"NVIDIA_API_KEY\")\n",
        "\n",
        "# Print a message based on whether the key was found\n",
        "if api_key:\n",
        "    print(\"NVIDIA API key loaded successfully\")\n",
        "else:\n",
        "    raise ValueError(\"NVIDIA API key not found in .env file. Please set NVIDIA_API_KEY in your environment variables.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJ_IwK8_-dSY",
        "outputId": "3b008383-16a0-4c5d-e66e-f6766f511d09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 20 of 88 available models:\n",
            "1. id='institute-of-science-tokyo/llama-3.1-swallow-70b-instruct-v0.1' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=True base_model=None\n",
            "2. id='nvidia/llama-3.1-nemotron-51b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "3. id='qwen/qwen2.5-coder-32b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "4. id='google/gemma-7b' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'] supports_tools=False supports_structured_output=False base_model=None\n",
            "5. id='yentinglin/llama-3-taiwan-70b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "6. id='microsoft/kosmos-2' model_type='nv-vlm' client='ChatNVIDIA' endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2' aliases=['ai-microsoft-kosmos-2', 'playground_kosmos_2', 'kosmos_2'] supports_tools=False supports_structured_output=False base_model=None\n",
            "7. id='mistralai/mamba-codestral-7b-v0.1' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "8. id='google/gemma-2b' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-gemma-2b', 'playground_gemma_2b', 'gemma_2b'] supports_tools=False supports_structured_output=False base_model=None\n",
            "9. id='seallms/seallm-7b-v2.5' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-seallm-7b'] supports_tools=False supports_structured_output=False base_model=None\n",
            "10. id='microsoft/phi-3.5-moe-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "11. id='microsoft/phi-3-mini-4k-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'] supports_tools=False supports_structured_output=False base_model=None\n",
            "12. id='meta/llama-3.2-11b-vision-instruct' model_type='vlm' client='ChatNVIDIA' endpoint='https://ai.api.nvidia.com/v1/gr/meta/llama-3.2-11b-vision-instruct/chat/completions' aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "13. id='mistralai/mixtral-8x22b-instruct-v0.1' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-mixtral-8x22b-instruct'] supports_tools=False supports_structured_output=False base_model=None\n",
            "14. id='microsoft/phi-3-medium-128k-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-phi-3-medium-128k-instruct'] supports_tools=False supports_structured_output=False base_model=None\n",
            "15. id='rakuten/rakutenai-7b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "16. id='meta/llama-3.2-3b-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=True supports_structured_output=True base_model=None\n",
            "17. id='databricks/dbrx-instruct' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-dbrx-instruct'] supports_tools=False supports_structured_output=False base_model=None\n",
            "18. id='nvidia/llama3-chatqa-1.5-70b' model_type='qa' client='ChatNVIDIA' endpoint=None aliases=['ai-chatqa-1.5-70b'] supports_tools=False supports_structured_output=False base_model=None\n",
            "19. id='nvidia/llama-3.1-nemotron-70b-reward' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=None supports_tools=False supports_structured_output=False base_model=None\n",
            "20. id='writer/palmyra-med-70b' model_type='chat' client='ChatNVIDIA' endpoint=None aliases=['ai-palmyra-med-70b'] supports_tools=False supports_structured_output=False base_model=None\n"
          ]
        }
      ],
      "source": [
        "### CODE BLOCK 3\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "\n",
        "# Get all available models\n",
        "all_models = ChatNVIDIA.get_available_models()\n",
        "\n",
        "# Display only the first 20 models\n",
        "print(f\"First 20 of {len(all_models)} available models:\")\n",
        "for i, model in enumerate(all_models[:20]):\n",
        "    print(f\"{i+1}. {model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUaPrwDVkb9p"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document split into 83 chunks\n"
          ]
        }
      ],
      "source": [
        "### CODE BLOCK 4\n",
        "###################################### Document Summarization  ##################################\n",
        "\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredFileLoader, ArxivLoader\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List\n",
        "from IPython.display import clear_output\n",
        "from functools import partial\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "# Define the Pydantic model for document summarization\n",
        "class DocumentSummaryBase(BaseModel):\n",
        "    \"\"\"Base class for document summarization results.\"\"\"\n",
        "    running_summary: str = Field(default=\"\", description=\"The running summary of the document. Update, don't override.\")\n",
        "    main_ideas: List[str] = Field(default_factory=list, description=\"Maximum 3 important points from the document.\")\n",
        "    loose_ends: List[str] = Field(default_factory=list, description=\"Maximum 3 open questions or areas needing clarification.\")\n",
        "\n",
        "# Set up console formatting with NVIDIA brand color\n",
        "console = Console()\n",
        "nvidia_green = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=nvidia_green)\n",
        "\n",
        "# Document Loading \n",
        "# For local files (commented out as an option)\n",
        "# loader = UnstructuredFileLoader(\"your_document.pdf\")\n",
        "# docs = loader.load()\n",
        "\n",
        "# Load document from Arxiv (GraphRAG paper)\n",
        "loader = ArxivLoader(query=\"2404.16130\")  # GraphRAG paper\n",
        "docs = loader.load()\n",
        "\n",
        "# Configure text splitter for document chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1200, \n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
        ")\n",
        "\n",
        "# Optional preprocessing (commented out)\n",
        "# def preprocess_text(text):\n",
        "#     # Remove special characters, normalize whitespace, etc.\n",
        "#     return text.replace(\"...\", \".\")\n",
        "\n",
        "# Split documents into manageable chunks\n",
        "docs_split = text_splitter.split_documents(docs)\n",
        "print(f\"Document split into {len(docs_split)} chunks\")\n",
        "\n",
        "# Create summarization prompt\n",
        "summary_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "    You are a technical document summarizer tasked with generating a running summary of a research document.\n",
        "    You are given chunks of text from a document and an existing knowledge base containing:\n",
        "    1. A running summary of the document so far\n",
        "    2. Main ideas (max 3)\n",
        "    3. Loose ends or questions (max 3)\n",
        "    \n",
        "    Your goal is to update this knowledge base with new information from the current document chunk.\n",
        "    \n",
        "    IMPORTANT INSTRUCTIONS:\n",
        "    - DO NOT lose information when updating the knowledge base\n",
        "    - Integrate new information with existing running_summary\n",
        "    - Update main_ideas and loose_ends as needed, but keep them limited to 3 items each\n",
        "    - Follow the format instructions exactly\n",
        "    - If nothing new is present in the chunk, return the existing knowledge base unchanged\n",
        "    \n",
        "    {format_instructions}\n",
        "    \"\"\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6zwDYqxlSkK"
      },
      "outputs": [],
      "source": [
        "### CODE BLOCK 5\n",
        "\n",
        "def RExtract(pydantic_class, llm, prompt):\n",
        "    '''\n",
        "    Runnable Extraction module\n",
        "    Returns a knowledge dictionary populated by slot-filling extraction\n",
        "    '''\n",
        "    parser = PydanticOutputParser(pydantic_object=pydantic_class)\n",
        "    instruct_merge = RunnableAssign({'format_instructions' : lambda x: parser.get_format_instructions()})\n",
        "    def preparse(string):\n",
        "        if '{' not in string: string = '{' + string\n",
        "        if '}' not in string: string = string + '}'\n",
        "        string = (string\n",
        "            .replace(\"\\\\_\", \"_\")\n",
        "            .replace(\"\\n\", \" \")\n",
        "            .replace(r\"\\]\", \"]\")\n",
        "            .replace(r\"\\[\", \"[\")\n",
        "        )\n",
        "        return string\n",
        "    return instruct_merge | prompt | llm | preparse | parser\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 701
        },
        "id": "7MRbsFh2lMx5",
        "outputId": "38414c07-9a3f-4b72-e147-3249294853df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{   \"description\": \"Base class for document summarization results.\",   \"properties\": {     \"running_summary\": \"The document discusses a new method called GraphRAG that creates summaries for nested modular communities in a graph. It also mentions the necessity of adaptive benchmarking for evaluating the performance of this method.\",     \"main_ideas\": [\"GraphRAG for creating summaries for nested modular communities\", \"Adaptive benchmarking for evaluating GraphRAG\"],     \"loose_ends\": [\"Details about the LLM used in GraphRAG\", \"More information about the proposed method for generating questions for evaluation\", \"Information about other benchmark datasets and their focus on vector RAG performance\"]   } }\n",
            "Considered 10 documents\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DocumentSummaryBase</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">running_summary</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">''</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">main_ideas</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[], </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">loose_ends</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[])</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;35mDocumentSummaryBase\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mrunning_summary\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m''\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mmain_ideas\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mloose_ends\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### CODE BLOCK 6\n",
        "\n",
        "latest_summary = \"\"\n",
        "\n",
        "def RSummarizer(knowledge, llm, prompt, verbose=False):\n",
        "    def summarize_docs(docs):\n",
        "        parse_chain = RunnableAssign({'info_base' : RExtract(knowledge.__class__, llm, prompt)})\n",
        "        state = {'info_base' : knowledge}\n",
        "\n",
        "        global latest_summary  ## If your loop crashes, you can check out the latest_summary\n",
        "\n",
        "        for i, doc in enumerate(docs):\n",
        "            state['input'] = doc.page_content\n",
        "            state = parse_chain.invoke(state)\n",
        "\n",
        "            assert 'info_base' in state\n",
        "            if verbose:\n",
        "                print(f\"Considered {i+1} documents\")\n",
        "                pprint(state['info_base'])\n",
        "                latest_summary = state['info_base']\n",
        "                clear_output(wait=True)\n",
        "\n",
        "        return state['info_base']\n",
        "    return RunnableLambda(summarize_docs)\n",
        "\n",
        "instruct_model = ChatNVIDIA(model=\"mistralai/mistral-7b-instruct-v0.3\").bind(max_tokens=4096)\n",
        "instruct_llm = instruct_model | StrOutputParser()\n",
        "\n",
        "## Take the first 10 document chunks and accumulate a DocumentSummaryBase\n",
        "summarizer = RSummarizer(DocumentSummaryBase(), instruct_llm, summary_prompt, verbose=True)\n",
        "summary = summarizer.invoke(docs_split[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MxWRyjbREz7e",
        "outputId": "d6770fd9-31ca-4b74-a23c-dfad4897a28d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dbenn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading Documents\n",
            "Loaded: 1706.03762 - Attention Is All You Need\n",
            "Loaded: 1810.04805 - BERT\n",
            "Loaded: 2005.11401 - RAG\n",
            "Loaded: 2205.00445 - MRKL\n",
            "Loaded: 2310.06825 - Mistral\n",
            "Loaded: 2306.05685 - LLM-as-a-Judge\n",
            "Loaded: 2210.03629 - ReAct\n",
            "Loaded: 2112.10752 - Latent Stable Diffusion\n",
            "Loaded: 2103.00020 - CLIP\n",
            "Loaded 9 documents\n",
            "Chunking Documents\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Available Documents:</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">• Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Attention Is All You Need</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">• Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - BERT</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">• Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - RAG</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">• Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - MRKL</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">• Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Mistral</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">• Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - LLM-as-a-Judge</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">• Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - ReAct</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">• Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - Latent Stable Diffusion</span>\n",
              "<span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">• Document </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\"> - CLIP</span>\n",
              "\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mAvailable Documents:\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m• Document \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;38;2;118;185;0m - Attention Is All You Need\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m• Document \u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;38;2;118;185;0m - BERT\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m• Document \u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;38;2;118;185;0m - RAG\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m• Document \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;38;2;118;185;0m - MRKL\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m• Document \u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;38;2;118;185;0m - Mistral\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m• Document \u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;38;2;118;185;0m - LLM-as-a-Judge\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m• Document \u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;38;2;118;185;0m - ReAct\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m• Document \u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;38;2;118;185;0m - Latent Stable Diffusion\u001b[0m\n",
              "\u001b[1;38;2;118;185;0m• Document \u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;38;2;118;185;0m - CLIP\u001b[0m\n",
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Document 1:\n",
            "  Chunks: 34\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">  Metadata: {</span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-08-02'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ashish Vaswani, Noam </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">encoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">attention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">show these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">existing best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">our model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">GPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">generalizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">limited training data.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Attention Is All You Need'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m  Metadata: \u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-08-02'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Attention Is All You Need'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ashish Vaswani, Noam \u001b[0m\n",
              "\u001b[32mShazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\n",
              "\u001b[32m'The dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks in an \u001b[0m\n",
              "\u001b[32mencoder-decoder configuration. The best\\nperforming models also connect the encoder and decoder through an \u001b[0m\n",
              "\u001b[32mattention\\nmechanism. We propose a new simple network architecture, the Transformer, based\\nsolely on attention \u001b[0m\n",
              "\u001b[32mmechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks \u001b[0m\n",
              "\u001b[32mshow these models to be\\nsuperior in quality while being more parallelizable and requiring significantly\\nless time\u001b[0m\n",
              "\u001b[32mto train. Our model achieves 28.4 BLEU on the WMT 2014\\nEnglish-to-German translation task, improving over the \u001b[0m\n",
              "\u001b[32mexisting best results,\\nincluding ensembles by over 2 BLEU. On the WMT 2014 English-to-French\\ntranslation task, \u001b[0m\n",
              "\u001b[32mour model establishes a new single-model state-of-the-art\\nBLEU score of 41.8 after training for 3.5 days on eight \u001b[0m\n",
              "\u001b[32mGPUs, a small fraction\\nof the training costs of the best models from the literature. We show that the\\nTransformer\u001b[0m\n",
              "\u001b[32mgeneralizes well to other tasks by applying it successfully to\\nEnglish constituency parsing both with large and \u001b[0m\n",
              "\u001b[32mlimited training data.'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'description'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Attention Is All You Need'\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 2:\n",
            "  Chunks: 42\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">  Metadata: {</span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2019-05-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT: Pre-training of Deep Bidirectional Transformers for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Language Understanding'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">introduce a new language representation model called BERT, which stands\\nfor Bidirectional Encoder Representations </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to pre-train deep\\nbidirectional</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">representations from unlabeled text by jointly conditioning on\\nboth left and right context in all layers. As a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output layer to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language inference, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and empirically </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, including\\npushing </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement),</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">83.1\\n(5.1 point absolute improvement).'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'BERT'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m  Metadata: \u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2019-05-24'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'BERT: Pre-training of Deep Bidirectional Transformers for \u001b[0m\n",
              "\u001b[32mLanguage Understanding'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We \u001b[0m\n",
              "\u001b[32mintroduce a new language representation model called BERT, which stands\\nfor Bidirectional Encoder Representations \u001b[0m\n",
              "\u001b[32mfrom Transformers. Unlike recent\\nlanguage representation models, BERT is designed to pre-train deep\\nbidirectional\u001b[0m\n",
              "\u001b[32mrepresentations from unlabeled text by jointly conditioning on\\nboth left and right context in all layers. As a \u001b[0m\n",
              "\u001b[32mresult, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output layer to \u001b[0m\n",
              "\u001b[32mcreate\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language inference, \u001b[0m\n",
              "\u001b[32mwithout substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and empirically \u001b[0m\n",
              "\u001b[32mpowerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, including\\npushing \u001b[0m\n",
              "\u001b[32mthe GLUE score to 80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, MultiNLI\\naccuracy to 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\u001b[0m\n",
              "\u001b[32mSQuAD v1.1 question answering\\nTest F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD v2.0 Test F1 to \u001b[0m\n",
              "\u001b[32m83.1\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'description'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'BERT'\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 3:\n",
            "  Chunks: 42\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">  Metadata: {</span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-04-12'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Retrieval-Augmented Generation for Knowledge-Intensive NLP </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Tasks'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Large </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">state-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and precisely </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">manipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags behind </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">task-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their world </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism to\\nexplicit </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">non-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive downstream </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">tasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation (RAG) -- models </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">which\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG models </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">where the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">index\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">passages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">retrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">specific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">:</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'RAG'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m  Metadata: \u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-04-12'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Retrieval-Augmented Generation for Knowledge-Intensive NLP \u001b[0m\n",
              "\u001b[32mTasks'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, \u001b[0m\n",
              "\u001b[32mHeinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, Sebastian Riedel, Douwe Kiela'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Large \u001b[0m\n",
              "\u001b[32mpre-trained language models have been shown to store factual knowledge\\nin their parameters, and achieve \u001b[0m\n",
              "\u001b[32mstate-of-the-art results when fine-tuned on\\ndownstream NLP tasks. However, their ability to access and precisely \u001b[0m\n",
              "\u001b[32mmanipulate\\nknowledge is still limited, and hence on knowledge-intensive tasks, their\\nperformance lags behind \u001b[0m\n",
              "\u001b[32mtask-specific architectures. Additionally, providing\\nprovenance for their decisions and updating their world \u001b[0m\n",
              "\u001b[32mknowledge remain open\\nresearch problems. Pre-trained models with a differentiable access mechanism to\\nexplicit \u001b[0m\n",
              "\u001b[32mnon-parametric memory can overcome this issue, but have so far been\\nonly investigated for extractive downstream \u001b[0m\n",
              "\u001b[32mtasks. We explore a general-purpose\\nfine-tuning recipe for retrieval-augmented generation \u001b[0m\u001b[32m(\u001b[0m\u001b[32mRAG\u001b[0m\u001b[32m)\u001b[0m\u001b[32m -- models \u001b[0m\n",
              "\u001b[32mwhich\\ncombine pre-trained parametric and non-parametric memory for language\\ngeneration. We introduce RAG models \u001b[0m\n",
              "\u001b[32mwhere the parametric memory is a\\npre-trained seq2seq model and the non-parametric memory is a dense vector \u001b[0m\n",
              "\u001b[32mindex\\nof Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG\\nformulations, one which \u001b[0m\n",
              "\u001b[32mconditions on the same retrieved passages across the\\nwhole generated sequence, the other can use different \u001b[0m\n",
              "\u001b[32mpassages per token. We\\nfine-tune and evaluate our models on a wide range of knowledge-intensive NLP\\ntasks and set\u001b[0m\n",
              "\u001b[32mthe state-of-the-art on three open domain QA tasks, outperforming\\nparametric seq2seq models and task-specific \u001b[0m\n",
              "\u001b[32mretrieve-and-extract architectures.\\nFor language generation tasks, we find that RAG models generate more \u001b[0m\n",
              "\u001b[32mspecific,\\ndiverse and factual language than a state-of-the-art parametric-only seq2seq\\nbaseline.'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'description'\u001b[0m\u001b[1;38;2;118;185;0m:\u001b[0m\n",
              "\u001b[32m'RAG'\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 4:\n",
            "  Chunks: 37\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">  Metadata: {</span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-05-01'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL Systems: A modular, neuro-symbolic architecture that </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">combines large language models, external knowledge sources and discrete reasoning'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Ehud Karpas, Omri </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Leyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Tenenholtz'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Huge language models (LMs) have ushered in a new era for AI, serving as a\\ngateway to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">natural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">limited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">systems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">linguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Knowledge and Language (MRKL, pronounced \"miracle\") system,\\nsome of the technical challenges in implementing it, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'MRKL'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m  Metadata: \u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-05-01'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'MRKL Systems: A modular, neuro-symbolic architecture that \u001b[0m\n",
              "\u001b[32mcombines large language models, external knowledge sources and discrete reasoning'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Ehud Karpas, Omri \u001b[0m\n",
              "\u001b[32mAbend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin \u001b[0m\n",
              "\u001b[32mLeyton-Brown, Dor Muhlgay, Noam Rozen, Erez Schwartz, Gal Shachaf, Shai Shalev-Shwartz, Amnon Shashua, Moshe \u001b[0m\n",
              "\u001b[32mTenenholtz'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Huge language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have ushered in a new era for AI, serving as a\\ngateway to \u001b[0m\n",
              "\u001b[32mnatural-language-based knowledge tasks. Although an essential\\nelement of modern AI, LMs are also inherently \u001b[0m\n",
              "\u001b[32mlimited in a number of ways. We\\ndiscuss these limitations and how they can be avoided by adopting a \u001b[0m\n",
              "\u001b[32msystems\\napproach. Conceptualizing the challenge as one that involves knowledge and\\nreasoning in addition to \u001b[0m\n",
              "\u001b[32mlinguistic processing, we define a flexible\\narchitecture with multiple neural models, complemented by discrete \u001b[0m\n",
              "\u001b[32mknowledge\\nand reasoning modules. We describe this neuro-symbolic architecture, dubbed the\\nModular Reasoning, \u001b[0m\n",
              "\u001b[32mKnowledge and Language \u001b[0m\u001b[32m(\u001b[0m\u001b[32mMRKL, pronounced \"miracle\"\u001b[0m\u001b[32m)\u001b[0m\u001b[32m system,\\nsome of the technical challenges in implementing it, \u001b[0m\n",
              "\u001b[32mand Jurassic-X, AI21 Labs\\'\\nMRKL system implementation.'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'description'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'MRKL'\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 5:\n",
            "  Chunks: 20\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">  Metadata: {</span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-10-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral 7B'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Albert Q. Jiang, Alexandre Sablayrolles,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Lavril, Thomas Wang, Timothée Lacroix, William El Sayed'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'We introduce Mistral 7B v0.1, a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">7-billion-parameter language model engineered\\nfor superior performance and efficiency. Mistral 7B outperforms </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Llama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\ncode generation. Our </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">model leverages grouped-query attention (GQA) for faster\\ninference, coupled with sliding window attention (SWA) to</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">effectively handle\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\nmodel </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\nthe Llama 2 13B -- Chat model both on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">human and automated benchmarks. Our\\nmodels are released under the Apache 2.0 license.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Mistral'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m  Metadata: \u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-10-10'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Mistral 7B'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Albert Q. Jiang, Alexandre Sablayrolles,\u001b[0m\n",
              "\u001b[32mArthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, \u001b[0m\n",
              "\u001b[32mGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut \u001b[0m\n",
              "\u001b[32mLavril, Thomas Wang, Timothée Lacroix, William El Sayed'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'We introduce Mistral 7B v0.1, a \u001b[0m\n",
              "\u001b[32m7-billion-parameter language model engineered\\nfor superior performance and efficiency. Mistral 7B outperforms \u001b[0m\n",
              "\u001b[32mLlama 2 13B\\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\\ncode generation. Our \u001b[0m\n",
              "\u001b[32mmodel leverages grouped-query attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mGQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m for faster\\ninference, coupled with sliding window attention \u001b[0m\u001b[32m(\u001b[0m\u001b[32mSWA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m to\u001b[0m\n",
              "\u001b[32meffectively handle\\nsequences of arbitrary length with a reduced inference cost. We also provide a\\nmodel \u001b[0m\n",
              "\u001b[32mfine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\\nthe Llama 2 13B -- Chat model both on \u001b[0m\n",
              "\u001b[32mhuman and automated benchmarks. Our\\nmodels are released under the Apache 2.0 license.'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'description'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Mistral'\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 6:\n",
            "  Chunks: 43\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">  Metadata: {</span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-12-24'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Evaluating large language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">model (LLM) based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">benchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">these models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">position, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">mitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">benchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">achieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">we show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">available at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'LLM-as-a-Judge'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m  Metadata: \u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-12-24'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
              "\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan \u001b[0m\n",
              "\u001b[32mLi, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Evaluating large language \u001b[0m\n",
              "\u001b[32mmodel \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLM\u001b[0m\u001b[32m)\u001b[0m\u001b[32m based chat assistants is challenging\\ndue to their broad capabilities and the inadequacy of existing \u001b[0m\n",
              "\u001b[32mbenchmarks in\\nmeasuring human preferences. To address this, we explore using strong LLMs as\\njudges to evaluate \u001b[0m\n",
              "\u001b[32mthese models on more open-ended questions. We examine the\\nusage and limitations of LLM-as-a-judge, including \u001b[0m\n",
              "\u001b[32mposition, verbosity, and\\nself-enhancement biases, as well as limited reasoning ability, and propose\\nsolutions to \u001b[0m\n",
              "\u001b[32mmitigate some of them. We then verify the agreement between LLM\\njudges and human preferences by introducing two \u001b[0m\n",
              "\u001b[32mbenchmarks: MT-bench, a\\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\\nresults \u001b[0m\n",
              "\u001b[32mreveal that strong LLM judges like GPT-4 can match both controlled and\\ncrowdsourced human preferences well, \u001b[0m\n",
              "\u001b[32machieving over 80% agreement, the same\\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable \u001b[0m\n",
              "\u001b[32mand\\nexplainable way to approximate human preferences, which are otherwise very\\nexpensive to obtain. Additionally,\u001b[0m\n",
              "\u001b[32mwe show our benchmark and traditional\\nbenchmarks complement each other by evaluating several variants of LLaMA \u001b[0m\n",
              "\u001b[32mand\\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\\nhuman preferences are publicly \u001b[0m\n",
              "\u001b[32mavailable at\\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'description'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'LLM-as-a-Judge'\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 7:\n",
            "  Chunks: 48\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">  Metadata: {</span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2023-03-10'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'ReAct: Synergizing Reasoning and Acting in Language Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'While large language models (LLMs) have demonstrated impressive capabilities\\nacross tasks in language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">understanding and interactive decision making, their\\nabilities for reasoning (e.g. chain-of-thought prompting) and</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">acting (e.g.\\naction plan generation) have primarily been studied as separate topics. In this\\npaper, we explore </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">greater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">as handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">environments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">human interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">question answering (HotpotQA) and fact verification (Fever),\\nReAct overcomes issues of hallucination and error </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">propagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">human-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">interactive decision making\\nbenchmarks (ALFWorld and WebShop), ReAct outperforms imitation and\\nreinforcement </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">learning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">two in-context examples.\\nProject site with code: https://react-lm.github.io'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'ReAct'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m  Metadata: \u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2023-03-10'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'ReAct: Synergizing Reasoning and Acting in Language Models'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
              "\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, Yuan Cao'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\n",
              "\u001b[32m'While large language models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLLMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have demonstrated impressive capabilities\\nacross tasks in language \u001b[0m\n",
              "\u001b[32munderstanding and interactive decision making, their\\nabilities for reasoning \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g. chain-of-thought prompting\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and\u001b[0m\n",
              "\u001b[32macting \u001b[0m\u001b[32m(\u001b[0m\u001b[32me.g.\\naction plan generation\u001b[0m\u001b[32m)\u001b[0m\u001b[32m have primarily been studied as separate topics. In this\\npaper, we explore \u001b[0m\n",
              "\u001b[32mthe use of LLMs to generate both reasoning traces and\\ntask-specific actions in an interleaved manner, allowing for\u001b[0m\n",
              "\u001b[32mgreater synergy\\nbetween the two: reasoning traces help the model induce, track, and update\\naction plans as well \u001b[0m\n",
              "\u001b[32mas handle exceptions, while actions allow it to interface\\nwith external sources, such as knowledge bases or \u001b[0m\n",
              "\u001b[32menvironments, to gather\\nadditional information. We apply our approach, named ReAct, to a diverse set of\\nlanguage \u001b[0m\n",
              "\u001b[32mand decision making tasks and demonstrate its effectiveness over\\nstate-of-the-art baselines, as well as improved \u001b[0m\n",
              "\u001b[32mhuman interpretability and\\ntrustworthiness over methods without reasoning or acting components.\\nConcretely, on \u001b[0m\n",
              "\u001b[32mquestion answering \u001b[0m\u001b[32m(\u001b[0m\u001b[32mHotpotQA\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and fact verification \u001b[0m\u001b[32m(\u001b[0m\u001b[32mFever\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\nReAct overcomes issues of hallucination and error \u001b[0m\n",
              "\u001b[32mpropagation prevalent in\\nchain-of-thought reasoning by interacting with a simple Wikipedia API, and\\ngenerates \u001b[0m\n",
              "\u001b[32mhuman-like task-solving trajectories that are more interpretable than\\nbaselines without reasoning traces. On two \u001b[0m\n",
              "\u001b[32minteractive decision making\\nbenchmarks \u001b[0m\u001b[32m(\u001b[0m\u001b[32mALFWorld and WebShop\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, ReAct outperforms imitation and\\nreinforcement \u001b[0m\n",
              "\u001b[32mlearning methods by an absolute success rate of 34% and 10%\\nrespectively, while being prompted with only one or \u001b[0m\n",
              "\u001b[32mtwo in-context examples.\\nProject site with code: https://react-lm.github.io'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'description'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'ReAct'\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 8:\n",
            "  Chunks: 48\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">  Metadata: {</span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2022-04-13'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'High-Resolution Image Synthesis with Latent Diffusion Models'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'By </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">decomposing the image formation process into a sequential application of\\ndenoising autoencoders, diffusion models </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">(DMs) achieve state-of-the-art\\nsynthesis results on image data and beyond. Additionally, their formulation\\nallows</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">for a guiding mechanism to control the image generation process without\\nretraining. However, since these models </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">typically operate directly in pixel\\nspace, optimization of powerful DMs often consumes hundreds of GPU days </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and\\ninference is expensive due to sequential evaluations. To enable DM training on\\nlimited computational </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">resources while retaining their quality and flexibility,\\nwe apply them in the latent space of powerful pretrained </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">autoencoders. In\\ncontrast to previous work, training diffusion models on such a representation\\nallows for the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">first time to reach a near-optimal point between complexity\\nreduction and detail preservation, greatly boosting </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">visual fidelity. By\\nintroducing cross-attention layers into the model architecture, we turn\\ndiffusion models into</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">powerful and flexible generators for general conditioning\\ninputs such as text or bounding boxes and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">high-resolution synthesis becomes\\npossible in a convolutional manner. Our latent diffusion models (LDMs) </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">achieve\\na new state of the art for image inpainting and highly competitive performance\\non various tasks, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">including unconditional image generation, semantic scene\\nsynthesis, and super-resolution, while significantly </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">reducing computational\\nrequirements compared to pixel-based DMs. Code is available </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">at\\nhttps://github.com/CompVis/latent-diffusion .'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Latent Stable Diffusion'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m  Metadata: \u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2022-04-13'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'High-Resolution Image Synthesis with Latent Diffusion Models'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
              "\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, Björn Ommer'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'By \u001b[0m\n",
              "\u001b[32mdecomposing the image formation process into a sequential application of\\ndenoising autoencoders, diffusion models \u001b[0m\n",
              "\u001b[32m(\u001b[0m\u001b[32mDMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m achieve state-of-the-art\\nsynthesis results on image data and beyond. Additionally, their formulation\\nallows\u001b[0m\n",
              "\u001b[32mfor a guiding mechanism to control the image generation process without\\nretraining. However, since these models \u001b[0m\n",
              "\u001b[32mtypically operate directly in pixel\\nspace, optimization of powerful DMs often consumes hundreds of GPU days \u001b[0m\n",
              "\u001b[32mand\\ninference is expensive due to sequential evaluations. To enable DM training on\\nlimited computational \u001b[0m\n",
              "\u001b[32mresources while retaining their quality and flexibility,\\nwe apply them in the latent space of powerful pretrained \u001b[0m\n",
              "\u001b[32mautoencoders. In\\ncontrast to previous work, training diffusion models on such a representation\\nallows for the \u001b[0m\n",
              "\u001b[32mfirst time to reach a near-optimal point between complexity\\nreduction and detail preservation, greatly boosting \u001b[0m\n",
              "\u001b[32mvisual fidelity. By\\nintroducing cross-attention layers into the model architecture, we turn\\ndiffusion models into\u001b[0m\n",
              "\u001b[32mpowerful and flexible generators for general conditioning\\ninputs such as text or bounding boxes and \u001b[0m\n",
              "\u001b[32mhigh-resolution synthesis becomes\\npossible in a convolutional manner. Our latent diffusion models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mLDMs\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32machieve\\na new state of the art for image inpainting and highly competitive performance\\non various tasks, \u001b[0m\n",
              "\u001b[32mincluding unconditional image generation, semantic scene\\nsynthesis, and super-resolution, while significantly \u001b[0m\n",
              "\u001b[32mreducing computational\\nrequirements compared to pixel-based DMs. Code is available \u001b[0m\n",
              "\u001b[32mat\\nhttps://github.com/CompVis/latent-diffusion .'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'description'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Latent Stable Diffusion'\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Document 9:\n",
            "  Chunks: 143\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">  Metadata: {</span><span style=\"color: #008000; text-decoration-color: #008000\">'Published'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'2021-02-26'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Title'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Learning Transferable Visual Models From Natural Language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Supervision'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Authors'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'Summary'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">'State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined object categories. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">This restricted form of supervision limits\\ntheir generality and usability since additional labeled data is needed </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to\\nspecify any other visual concept. Learning directly from raw text about images\\nis a promising alternative </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">which leverages a much broader source of\\nsupervision. We demonstrate that the simple pre-training task of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">predicting\\nwhich caption goes with which image is an efficient and scalable way to learn\\nSOTA image </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">representations from scratch on a dataset of 400 million (image,\\ntext) pairs collected from the internet. After </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training, natural language\\nis used to reference learned visual concepts (or describe new ones) </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">enabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">benchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">recognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">transfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">for any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">ImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">our code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #008000; text-decoration-color: #008000\">'description'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">: </span><span style=\"color: #008000; text-decoration-color: #008000\">'CLIP'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">}</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0m  Metadata: \u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[32m'Published'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'2021-02-26'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Title'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Learning Transferable Visual Models From Natural Language \u001b[0m\n",
              "\u001b[32mSupervision'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Authors'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\u001b[0m\n",
              "\u001b[32mGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'Summary'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\n",
              "\u001b[32m'State-of-the-art computer vision systems are trained to predict a fixed set\\nof predetermined object categories. \u001b[0m\n",
              "\u001b[32mThis restricted form of supervision limits\\ntheir generality and usability since additional labeled data is needed \u001b[0m\n",
              "\u001b[32mto\\nspecify any other visual concept. Learning directly from raw text about images\\nis a promising alternative \u001b[0m\n",
              "\u001b[32mwhich leverages a much broader source of\\nsupervision. We demonstrate that the simple pre-training task of \u001b[0m\n",
              "\u001b[32mpredicting\\nwhich caption goes with which image is an efficient and scalable way to learn\\nSOTA image \u001b[0m\n",
              "\u001b[32mrepresentations from scratch on a dataset of 400 million \u001b[0m\u001b[32m(\u001b[0m\u001b[32mimage,\\ntext\u001b[0m\u001b[32m)\u001b[0m\u001b[32m pairs collected from the internet. After \u001b[0m\n",
              "\u001b[32mpre-training, natural language\\nis used to reference learned visual concepts \u001b[0m\u001b[32m(\u001b[0m\u001b[32mor describe new ones\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32menabling\\nzero-shot transfer of the model to downstream tasks. We study the performance\\nof this approach by \u001b[0m\n",
              "\u001b[32mbenchmarking on over 30 different existing computer vision\\ndatasets, spanning tasks such as OCR, action \u001b[0m\n",
              "\u001b[32mrecognition in videos,\\ngeo-localization, and many types of fine-grained object classification. The\\nmodel \u001b[0m\n",
              "\u001b[32mtransfers non-trivially to most tasks and is often competitive with a\\nfully supervised baseline without the need \u001b[0m\n",
              "\u001b[32mfor any dataset specific training.\\nFor instance, we match the accuracy of the original ResNet-50 on \u001b[0m\n",
              "\u001b[32mImageNet\\nzero-shot without needing to use any of the 1.28 million training examples it\\nwas trained on. We release\u001b[0m\n",
              "\u001b[32mour code and pre-trained model weights at\\nhttps://github.com/OpenAI/CLIP.'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[32m'description'\u001b[0m\u001b[1;38;2;118;185;0m: \u001b[0m\u001b[32m'CLIP'\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "### CODE BLOCK 7\n",
        "########################################### Retrieval-Augmented Generation (RAG) ###########################################\n",
        "\n",
        "# Standard library imports\n",
        "import json\n",
        "from operator import itemgetter\n",
        "\n",
        "# UI components\n",
        "import gradio as gr\n",
        "\n",
        "# NVIDIA specific components\n",
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA, NVIDIAEmbeddings\n",
        "\n",
        "# Vector storage components\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from faiss import IndexFlatL2\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "\n",
        "# Document processing components\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import ArxivLoader, UnstructuredPDFLoader\n",
        "\n",
        "# Document transformation\n",
        "from langchain.document_transformers import LongContextReorder\n",
        "\n",
        "# LangChain components\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables.passthrough import RunnableAssign\n",
        "\n",
        "# Utility imports\n",
        "from functools import partial\n",
        "\n",
        "# Rich console formatting\n",
        "from rich.console import Console\n",
        "from rich.style import Style\n",
        "from rich.theme import Theme\n",
        "\n",
        "# Set up console formatting with NVIDIA brand color\n",
        "console = Console()\n",
        "nvidia_green = Style(color=\"#76B900\", bold=True)\n",
        "pprint = partial(console.print, style=nvidia_green)\n",
        "\n",
        "# Configure embedding model\n",
        "# NVIDIAEmbeddings.get_available_models()  # Get list of available embedding models\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "\n",
        "# Set up LLM \n",
        "# ChatNVIDIA.get_available_models()  # Get list of available LLM models\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x22b-instruct-v0.1\")\n",
        "\n",
        "# Utility function for pretty printing in chains\n",
        "def RPrint(preface=None):\n",
        "    \"\"\"\n",
        "    Creates a RunnableLambda that prints input with NVIDIA styling and returns it unchanged.\n",
        "    Useful for debugging chains by showing intermediate values.\n",
        "    \n",
        "    Args:\n",
        "        preface: Optional text to print before the input\n",
        "        \n",
        "    Returns:\n",
        "        RunnableLambda: A function that prints then returns its input\n",
        "    \"\"\"\n",
        "    def print_and_return(x):\n",
        "        msg = f\"{preface}: {x}\" if preface else x\n",
        "        pprint(msg)\n",
        "        return x\n",
        "    \n",
        "    return RunnableLambda(print_and_return)\n",
        "\n",
        "# Function to format document chunks as a string\n",
        "def docs2str(docs, title=None):\n",
        "    \"\"\"\n",
        "    Formats a list of document chunks into a readable string format.\n",
        "    \n",
        "    Args:\n",
        "        docs: List of document objects with page_content and metadata\n",
        "        title: Optional title to include in the output\n",
        "        \n",
        "    Returns:\n",
        "        str: Formatted string representation of the documents\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return \"(No relevant documents found)\"\n",
        "    \n",
        "    result = f\"--- {title} ---\\n\\n\" if title else \"\"\n",
        "    \n",
        "    for i, doc in enumerate(docs):\n",
        "        source_info = \"\"\n",
        "        if hasattr(doc, 'metadata') and doc.metadata:\n",
        "            if 'title' in doc.metadata:\n",
        "                source_info = f\"[Quote from {doc.metadata['title']}]\"\n",
        "            elif 'source' in doc.metadata:\n",
        "                source_info = f\"[Quote from {doc.metadata['source']}]\"\n",
        "        \n",
        "        result += f\"{source_info}\\n{doc.page_content}\\n\\n\"\n",
        "    \n",
        "    return result\n",
        "\n",
        "# Create document transformer for reordering documents for better context\n",
        "long_reorder = RunnableLambda(lambda docs: LongContextReorder().transform_documents(docs))\n",
        "\n",
        "# Configure text splitter for document chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \"]\n",
        ")\n",
        "\n",
        "# Load academic papers using ArxivLoader\n",
        "print(\"Loading Documents\")\n",
        "documents = []\n",
        "arxiv_papers = [\n",
        "    (\"1706.03762\", \"Attention Is All You Need\"),  # Transformer architecture\n",
        "    (\"1810.04805\", \"BERT\"),  # Bidirectional Encoder Representations from Transformers\n",
        "    (\"2005.11401\", \"RAG\"),  # Retrieval-Augmented Generation\n",
        "    (\"2205.00445\", \"MRKL\"),  # Modular Reasoning, Knowledge and Language\n",
        "    (\"2310.06825\", \"Mistral\"),  # Mistral LLM\n",
        "    (\"2306.05685\", \"LLM-as-a-Judge\"),  # Using LLMs for evaluations\n",
        "    (\"2210.03629\", \"ReAct\"),  # Reasoning and Acting in LLMs\n",
        "    (\"2112.10752\", \"Latent Stable Diffusion\"),  # Diffusion Models\n",
        "    (\"2103.00020\", \"CLIP\")   # Contrastive Language-Image Pre-training\n",
        "]\n",
        "\n",
        "for paper_id, description in arxiv_papers:\n",
        "    try:\n",
        "        loader = ArxivLoader(query=paper_id)\n",
        "        doc = loader.load()\n",
        "        if doc:\n",
        "            # Add description to metadata\n",
        "            for d in doc:\n",
        "                if not hasattr(d, 'metadata'):\n",
        "                    d.metadata = {}\n",
        "                d.metadata['description'] = description\n",
        "            documents.extend(doc)\n",
        "            print(f\"Loaded: {paper_id} - {description}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load {paper_id}: {e}\")\n",
        "\n",
        "print(f\"Loaded {len(documents)} documents\")\n",
        "\n",
        "# Process the loaded papers\n",
        "# Convert to JSON and truncate at References section if present\n",
        "processed_docs = []\n",
        "for doc in documents:\n",
        "    content = doc.page_content\n",
        "    \n",
        "    # Truncate content at References section to focus on the main content\n",
        "    ref_indices = [content.find(\"\\nReferences\\n\"), content.find(\"\\nREFERENCES\\n\")]\n",
        "    ref_indices = [idx for idx in ref_indices if idx != -1]\n",
        "    \n",
        "    if ref_indices:\n",
        "        # Truncate at the first occurrence of References\n",
        "        content = content[:min(ref_indices)]\n",
        "    \n",
        "    doc.page_content = content\n",
        "    processed_docs.append(doc)\n",
        "\n",
        "# Split documents into chunks\n",
        "print(\"Chunking Documents\")\n",
        "docs_chunks = []\n",
        "for doc in processed_docs:\n",
        "    # Split document into chunks\n",
        "    chunks = text_splitter.split_documents([doc])\n",
        "    \n",
        "    # Filter out very short chunks (likely section headers or incomplete sentences)\n",
        "    filtered_chunks = [chunk for chunk in chunks if len(chunk.page_content) >= 200]\n",
        "    \n",
        "    docs_chunks.append(filtered_chunks)\n",
        "\n",
        "# Create metadata and summary information\n",
        "doc_string = \"Available Documents:\\n\"\n",
        "doc_metadata = []\n",
        "\n",
        "# Build document summary string and metadata\n",
        "for i, doc_chunk in enumerate(docs_chunks):\n",
        "    if not doc_chunk:  # Skip empty document chunks\n",
        "        continue\n",
        "    \n",
        "    # Extract document metadata\n",
        "    metadata = doc_chunk[0].metadata\n",
        "    title = metadata.get('title', f\"Document {i+1}\")\n",
        "    description = metadata.get('description', '')\n",
        "    \n",
        "    # Add to document string\n",
        "    doc_string += f\"• {title} - {description}\\n\"\n",
        "    \n",
        "    # Add to metadata\n",
        "    doc_metadata.append({\n",
        "        'index': i,\n",
        "        'title': title,\n",
        "        'description': description,\n",
        "        'chunk_count': len(doc_chunk)\n",
        "    })\n",
        "\n",
        "# Create extra chunks with document metadata\n",
        "extra_chunks = [\n",
        "    doc_string,\n",
        "    json.dumps(doc_metadata, indent=2)\n",
        "]\n",
        "\n",
        "# Print summary information\n",
        "pprint(doc_string)\n",
        "\n",
        "# Print detailed document information\n",
        "for i, chunks in enumerate(docs_chunks):\n",
        "    if not chunks:  # Skip empty document chunks\n",
        "        continue\n",
        "        \n",
        "    metadata = chunks[0].metadata\n",
        "    print(f\"Document {i+1}:\")\n",
        "    print(f\"  Chunks: {len(chunks)}\")\n",
        "    pprint(f\"  Metadata: {metadata}\")\n",
        "    print()  # Empty line for better readability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mz2AOChME7J_",
        "outputId": "54262dea-d13e-48fe-966a-cb54326217f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating vectorstore for metadata and summaries...\n",
            "Flattening document chunks...\n",
            "Total flattened document chunks: 457\n",
            "Creating vectorstore for document chunks...\n",
            "Aggregating vectorstores...\n",
            "Merged 2 vectorstores with 459 total vectors\n",
            "✅ Vector store construction complete with 459 total document chunks available for retrieval\n",
            "Note: Large vectorstores have been merged efficiently to minimize memory usage\n",
            "CPU times: total: 953 ms\n",
            "Wall time: 58.5 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "### CODE BLOCK 8\n",
        "\n",
        "# Constructing Your Document Vector Stores\n",
        "\n",
        "def default_FAISS(embedder):\n",
        "    \"\"\"\n",
        "    Create an empty FAISS vectorstore with proper configuration.\n",
        "    \n",
        "    Args:\n",
        "        embedder: The embedding model to use for determining vector dimensions\n",
        "        \n",
        "    Returns:\n",
        "        An empty FAISS vectorstore configured with IndexFlatL2 and InMemoryDocstore\n",
        "    \"\"\"\n",
        "    # Get the embedding dimension from a sample embedding\n",
        "    sample_embedding = embedder.embed_query(\"Sample text to determine dimensions\")\n",
        "    dimension = len(sample_embedding)\n",
        "    \n",
        "    # Create an empty FAISS index with L2 distance metric\n",
        "    index = IndexFlatL2(dimension)\n",
        "    \n",
        "    # Create an empty docstore and index mapping\n",
        "    docstore = InMemoryDocstore({})\n",
        "    index_to_docstore_id = {}\n",
        "    \n",
        "    # Return the configured FAISS instance\n",
        "    return FAISS(\n",
        "        embedding_function=embedder,\n",
        "        index=index,\n",
        "        docstore=docstore,\n",
        "        index_to_docstore_id=index_to_docstore_id,\n",
        "        normalize_L2=False  # Don't normalize vectors as per requirements\n",
        "    )\n",
        "\n",
        "def aggregate_vstores(vecstores):\n",
        "    \"\"\"\n",
        "    Efficiently merge multiple vector stores into a single vectorstore.\n",
        "    \n",
        "    Args:\n",
        "        vecstores: List of FAISS vectorstores to merge\n",
        "        \n",
        "    Returns:\n",
        "        A unified FAISS vectorstore containing all documents from input stores\n",
        "    \"\"\"\n",
        "    # Create an empty FAISS instance to serve as the aggregation target\n",
        "    # Using the embedding function from the first vectorstore\n",
        "    merged_store = default_FAISS(vecstores[0].embedding_function)\n",
        "    \n",
        "    # Track the total number of vectors for progress reporting\n",
        "    total_docs = 0\n",
        "    \n",
        "    # Iterate through each vectorstore and merge its contents\n",
        "    for vs in vecstores:\n",
        "        # Access the internal components of each FAISS instance\n",
        "        docstore_dict = vs.docstore._dict\n",
        "        \n",
        "        # Collect texts, metadatas, and ids for batch processing\n",
        "        texts = []\n",
        "        metadatas = []\n",
        "        ids = []\n",
        "        \n",
        "        # Process each document from the source vectorstore\n",
        "        for doc_id, doc in docstore_dict.items():\n",
        "            texts.append(doc.page_content)\n",
        "            metadatas.append(doc.metadata)\n",
        "            ids.append(doc_id)\n",
        "            total_docs += 1\n",
        "        \n",
        "        # Batch add all documents from this vectorstore\n",
        "        if texts:\n",
        "            merged_store.add_texts(texts, metadatas, ids)\n",
        "    \n",
        "    print(f\"Merged {len(vecstores)} vectorstores with {total_docs} total vectors\")\n",
        "    return merged_store\n",
        "\n",
        "# Create vector store from metadata/summary chunks\n",
        "print(\"Creating vectorstore for metadata and summaries...\")\n",
        "metadata_vectorstore = FAISS.from_texts(\n",
        "    extra_chunks,  # The summary and metadata text chunks\n",
        "    embedder,      # NVIDIA embeddings model\n",
        "    metadatas=[{\"source\": \"metadata\"} for _ in extra_chunks]  # Tag these as metadata\n",
        ")\n",
        "\n",
        "# Flatten the list of document chunks before creating the vector store\n",
        "# This fixes the \"list object has no attribute page_content\" error\n",
        "print(\"Flattening document chunks...\")\n",
        "flattened_docs_chunks = []\n",
        "for doc_chunk_list in docs_chunks:\n",
        "    flattened_docs_chunks.extend(doc_chunk_list)\n",
        "\n",
        "print(f\"Total flattened document chunks: {len(flattened_docs_chunks)}\")\n",
        "\n",
        "# Create vector store from document chunks\n",
        "print(\"Creating vectorstore for document chunks...\")\n",
        "docs_vectorstore = FAISS.from_documents(\n",
        "    flattened_docs_chunks,  # Now a flat list of document chunks\n",
        "    embedder               # NVIDIA embeddings model\n",
        ")\n",
        "\n",
        "# Combine the vectorstores\n",
        "vecstores = [metadata_vectorstore, docs_vectorstore]\n",
        "\n",
        "# Aggregate into a final docstore\n",
        "print(\"Aggregating vectorstores...\")\n",
        "docstore = aggregate_vstores(vecstores)\n",
        "\n",
        "# Print confirmation of total document count\n",
        "total_chunks = len(docstore.docstore._dict)  # Use _dict instead of dict\n",
        "print(f\"✅ Vector store construction complete with {total_chunks} total document chunks available for retrieval\")\n",
        "\n",
        "# Optional: Memory optimization hint\n",
        "print(\"Note: Large vectorstores have been merged efficiently to minimize memory usage\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_ZGftNJDFd3d",
        "outputId": "9ed73807-4500-48f8-ae3a-98e3d38371cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating conversation memory store...\n",
            "\n",
            "Testing chat implementation with query: Tell me about RAG!\n",
            "\n",
            "Response:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Retrieved Context for: Tell me about RAG!</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRetrieved Context for: Tell me about RAG!\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Prompt Sent to LLM: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"\\n    You are a knowledgeable research assistant who helps </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">users understand technical AI papers. \\n    \\n    The user's question is: Tell me about RAG!\\n    \\n    </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">CONVERSATION HISTORY:\\n    (No relevant documents found)\\n    \\n    DOCUMENT CONTEXT:\\n    --- Document Context </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">---\\n\\n\\nRAG-Token\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">transition probability: p′\\nθ(yi|x, y1:i−1) = P\\nz∈top-k(p(·|x)) pη(zi|x)pθ(yi|x, zi, y1:i−1) To\\ndecode, we can </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">plug p′\\nθ(yi|x, y1:i−1) into a standard beam decoder.\\nRAG-Sequence\\nFor RAG-Sequence, the likelihood p(y|x) does </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">not break into a conventional per-\\ntoken likelihood, hence we cannot solve it with a single beam search. Instead, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">we run beam search for\\neach document z, scoring each hypothesis using pθ(yi|x, z, y1:i−1). This yields a set of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">hypotheses\\nY , some of which may not have appeared in the beams of all documents. To estimate the probability\\nof </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">an hypothesis y we run an additional forward pass for each document z for which y does not\\nappear in the beam, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">multiply generator probability with pη(z|x) and then sum the probabilities across\\nbeams for the marginals. We </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">refer to this decoding procedure as “Thorough Decoding.” For longer\\n\\n\\nWe build RAG models where the parametric </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">memory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">accessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">end-to-end (Fig. 1). The\\nretriever (Dense Passage Retriever [26], henceforth DPR) provides latent documents </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conditioned on\\nthe input, and the seq2seq model (BART [32]) then conditions on these latent documents together </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">with\\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a per-output basis (assuming the same document is responsible for all tokens) or a per-token\\nbasis (where </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">different documents are responsible for different tokens). Like T5 [51] or BART, RAG\\ncan be ﬁne-tuned on any </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">seq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">proposing architectures to enrich systems with non-parametric\\n\\n\\nmemory by editing the document index. This </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">approach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">text directly, albeit obtained via TF-IDF\\nrather than end-to-end learnt retrieval [9].\\nRetrieve-and-Edit </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">approaches\\nOur method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">input-output pair is retrieved for a given input, and then edited\\nto provide a ﬁnal output. These approaches have </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">proved successful in a number of domains including\\nMachine Translation [18, 22] and Semantic Parsing [21]. Our </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">approach does have several differences,\\nincluding less of emphasis on lightly editing a retrieved item, but on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">aggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">evidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and\\ncould represent promising future work.\\n6\\nDiscussion\\n\\n\\nextractive tasks, we ﬁnd that unconstrained </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">generation outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">MS-MARCO [1] and Jeopardy question\\ngeneration, and we ﬁnd that our models generate responses that are more </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">factual, speciﬁc, and\\ndiverse than a BART baseline. For FEVER [56] fact veriﬁcation, we achieve results within </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">4.3% of\\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\\nthe</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">non-parametric memory can be replaced to update the models’ knowledge as the world changes.1\\n2\\nMethods\\nWe </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">explore RAG models, which use the input sequence x to retrieve text documents z and use them\\nas additional context</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">when generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: (i) a retriever </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pη(z|x) with parameters η that returns (top-K truncated)\\ndistributions over text passages given a query x and (ii)</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a generator pθ(yi|x, z, y1:i−1) parametrized\\n\\n\\ndocuments when producing an answer. Concretely, the top K </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">documents are retrieved using the\\nretriever, and then the generator produces a distribution for the next output </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">token for each document,\\nbefore marginalizing, and repeating the process with the following output token, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Formally, we deﬁne:\\npRAG-Token(y|x) ≈\\nN\\nY\\ni\\nX\\nz∈top-k(p(·|x))\\npη(z|x)pθ(yi|x, z, y1:i−1)\\nFinally, we note </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">that RAG can be used for sequence classiﬁcation tasks by considering the target class\\nas a target sequence of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">length one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2\\nRetriever: DPR\\nThe retrieval component </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pη(z|x) is based on DPR [26]. DPR follows a bi-encoder architecture:\\npη(z|x) ∝exp\\n\\x00d(z)⊤q(x)\\n\\x01\\nd(z) = </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">BERTd(z), q(x) = BERTq(x)\\nwhere d(z) is a dense representation of a document produced by a BERTBASE document </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">encoder [8],\\nand q(x) a query representation produced by a query encoder, also based on BERTBASE. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Calculating\\n\\n\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Library [66] and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/. An </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">interactive demo of RAG models can be found at https://huggingface.co/rag/\\n2\\nby θ that generates a current token </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">based on a context of the previous i −1 tokens y1:i−1, the original\\ninput x and a retrieved passage z.\\nTo train </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the retriever and generator end-to-end, we treat the retrieved document as a latent variable.\\nWe propose two </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">models that marginalize over the latent documents in different ways to produce a\\ndistribution over generated text.</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">In one approach, RAG-Sequence, the model uses the same document\\nto predict each target token. The second approach,</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">RAG-Token, can predict each target token based\\non a different document. In the following, we formally introduce </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">both models and then describe the\\npη and pθ components, as well as the training and decoding </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">procedure.\\n2.1\\nModels\\n\\n\\nbeams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">For longer\\noutput sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding,\\nwe</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">can make a further approximation that pθ(y|x, zi) ≈0 where y was not generated during beam\\nsearch from x, zi. This</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">avoids the need to run additional forward passes once the candidate set Y has\\nbeen generated. We refer to this </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">decoding procedure as “Fast Decoding.”\\n3\\nExperiments\\nWe experiment with RAG in a wide range of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">knowledge-intensive tasks. For all experiments, we use\\na single Wikipedia dump for our non-parametric knowledge </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">source. Following Lee et al. [31] and\\nKarpukhin et al. [26], we use the December 2018 dump. Each Wikipedia article</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">is split into disjoint\\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">an\\nembedding for each document, and build a single MIPS index using FAISS [23] with a Hierarchical\\n\\n\\npη and pθ </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">components, as well as the training and decoding procedure.\\n2.1\\nModels\\nRAG-Sequence Model\\nThe RAG-Sequence </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">model uses the same retrieved document to generate\\nthe complete sequence. Technically, it treats the retrieved </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">document as a single latent variable that\\nis marginalized to get the seq2seq probability p(y|x) via a top-K </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">approximation. Concretely, the\\ntop K documents are retrieved using the retriever, and the generator produces the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">output sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence(y|x) </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">≈\\nX\\nz∈top-k(p(·|x))\\npη(z|x)pθ(y|x, z) =\\nX\\nz∈top-k(p(·|x))\\npη(z|x)\\nN\\nY\\ni\\npθ(yi|x, z, y1:i−1)\\nRAG-Token </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Model\\nIn the RAG-Token model we can draw a different latent document for each\\ntarget token and marginalize </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">accordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Concretely, the top K documents are retrieved using the\\n\\n\\n    \\n    INSTRUCTIONS:\\n    1. Only answer using </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">information from the provided context and your understanding of AI concepts\\n    2. If you don't know or the answer</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">isn't in the context, say so honestly\\n    3. Keep responses conversational but technically precise\\n    4. Do not </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">make up information that isn't in the papers\\n    5. Include relevant quotes where helpful, citing the paper\\n    </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}), </span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'Tell me about RAG!'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}, </span>\n",
              "<span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={})]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mPrompt Sent to LLM: \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\\n    You are a knowledgeable research assistant who helps \u001b[0m\n",
              "\u001b[32musers understand technical AI papers. \\n    \\n    The user's question is: Tell me about RAG!\\n    \\n    \u001b[0m\n",
              "\u001b[32mCONVERSATION HISTORY:\\n    \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNo relevant documents found\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n    \\n    DOCUMENT CONTEXT:\\n    --- Document Context \u001b[0m\n",
              "\u001b[32m---\\n\\n\\nRAG-Token\\nThe RAG-Token model can be seen as a standard, autoregressive seq2seq genera-\\ntor with \u001b[0m\n",
              "\u001b[32mtransition probability: p′\\nθ\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, y1:i−1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m = P\\nz∈top-k\u001b[0m\u001b[32m(\u001b[0m\u001b[32mp\u001b[0m\u001b[32m(\u001b[0m\u001b[32m·|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m pη\u001b[0m\u001b[32m(\u001b[0m\u001b[32mzi|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32mpθ\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, zi, y1:i−1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m To\\ndecode, we can \u001b[0m\n",
              "\u001b[32mplug p′\\nθ\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, y1:i−1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m into a standard beam decoder.\\nRAG-Sequence\\nFor RAG-Sequence, the likelihood p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m does \u001b[0m\n",
              "\u001b[32mnot break into a conventional per-\\ntoken likelihood, hence we cannot solve it with a single beam search. Instead, \u001b[0m\n",
              "\u001b[32mwe run beam search for\\neach document z, scoring each hypothesis using pθ\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, z, y1:i−1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. This yields a set of \u001b[0m\n",
              "\u001b[32mhypotheses\\nY , some of which may not have appeared in the beams of all documents. To estimate the probability\\nof \u001b[0m\n",
              "\u001b[32man hypothesis y we run an additional forward pass for each document z for which y does not\\nappear in the beam, \u001b[0m\n",
              "\u001b[32mmultiply generator probability with pη\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and then sum the probabilities across\\nbeams for the marginals. We \u001b[0m\n",
              "\u001b[32mrefer to this decoding procedure as “Thorough Decoding.” For longer\\n\\n\\nWe build RAG models where the parametric \u001b[0m\n",
              "\u001b[32mmemory is a pre-trained seq2seq transformer, and the\\nnon-parametric memory is a dense vector index of Wikipedia, \u001b[0m\n",
              "\u001b[32maccessed with a pre-trained neural\\nretriever. We combine these components in a probabilistic model trained \u001b[0m\n",
              "\u001b[32mend-to-end \u001b[0m\u001b[32m(\u001b[0m\u001b[32mFig. 1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The\\nretriever \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDense Passage Retriever \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, henceforth DPR\u001b[0m\u001b[32m)\u001b[0m\u001b[32m provides latent documents \u001b[0m\n",
              "\u001b[32mconditioned on\\nthe input, and the seq2seq model \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBART \u001b[0m\u001b[32m[\u001b[0m\u001b[32m32\u001b[0m\u001b[32m]\u001b[0m\u001b[32m)\u001b[0m\u001b[32m then conditions on these latent documents together \u001b[0m\n",
              "\u001b[32mwith\\nthe input to generate the output. We marginalize the latent documents with a top-K approximation,\\neither on \u001b[0m\n",
              "\u001b[32ma per-output basis \u001b[0m\u001b[32m(\u001b[0m\u001b[32massuming the same document is responsible for all tokens\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or a per-token\\nbasis \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwhere \u001b[0m\n",
              "\u001b[32mdifferent documents are responsible for different tokens\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Like T5 \u001b[0m\u001b[32m[\u001b[0m\u001b[32m51\u001b[0m\u001b[32m]\u001b[0m\u001b[32m or BART, RAG\\ncan be ﬁne-tuned on any \u001b[0m\n",
              "\u001b[32mseq2seq task, whereby both the generator and retriever are jointly learned.\\nThere has been extensive previous work\u001b[0m\n",
              "\u001b[32mproposing architectures to enrich systems with non-parametric\\n\\n\\nmemory by editing the document index. This \u001b[0m\n",
              "\u001b[32mapproach has also been used in knowledge-intensive\\ndialog, where generators have been conditioned on retrieved \u001b[0m\n",
              "\u001b[32mtext directly, albeit obtained via TF-IDF\\nrather than end-to-end learnt retrieval \u001b[0m\u001b[32m[\u001b[0m\u001b[32m9\u001b[0m\u001b[32m]\u001b[0m\u001b[32m.\\nRetrieve-and-Edit \u001b[0m\n",
              "\u001b[32mapproaches\\nOur method shares some similarities with retrieve-and-edit style\\napproaches, where a similar training \u001b[0m\n",
              "\u001b[32minput-output pair is retrieved for a given input, and then edited\\nto provide a ﬁnal output. These approaches have \u001b[0m\n",
              "\u001b[32mproved successful in a number of domains including\\nMachine Translation \u001b[0m\u001b[32m[\u001b[0m\u001b[32m18, 22\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and Semantic Parsing \u001b[0m\u001b[32m[\u001b[0m\u001b[32m21\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. Our \u001b[0m\n",
              "\u001b[32mapproach does have several differences,\\nincluding less of emphasis on lightly editing a retrieved item, but on \u001b[0m\n",
              "\u001b[32maggregating content from several\\npieces of retrieved content, as well as learning latent retrieval, and retrieving\u001b[0m\n",
              "\u001b[32mevidence documents\\nrather than related training pairs. This said, RAG techniques may work well in these settings, \u001b[0m\n",
              "\u001b[32mand\\ncould represent promising future work.\\n6\\nDiscussion\\n\\n\\nextractive tasks, we ﬁnd that unconstrained \u001b[0m\n",
              "\u001b[32mgeneration outperforms previous extractive approaches.\\nFor knowledge-intensive generation, we experiment with \u001b[0m\n",
              "\u001b[32mMS-MARCO \u001b[0m\u001b[32m[\u001b[0m\u001b[32m1\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and Jeopardy question\\ngeneration, and we ﬁnd that our models generate responses that are more \u001b[0m\n",
              "\u001b[32mfactual, speciﬁc, and\\ndiverse than a BART baseline. For FEVER \u001b[0m\u001b[32m[\u001b[0m\u001b[32m56\u001b[0m\u001b[32m]\u001b[0m\u001b[32m fact veriﬁcation, we achieve results within \u001b[0m\n",
              "\u001b[32m4.3% of\\nstate-of-the-art pipeline models which use strong retrieval supervision. Finally, we demonstrate that\\nthe\u001b[0m\n",
              "\u001b[32mnon-parametric memory can be replaced to update the models’ knowledge as the world changes.1\\n2\\nMethods\\nWe \u001b[0m\n",
              "\u001b[32mexplore RAG models, which use the input sequence x to retrieve text documents z and use them\\nas additional context\u001b[0m\n",
              "\u001b[32mwhen generating the target sequence y. As shown in Figure 1, our models\\nleverage two components: \u001b[0m\u001b[32m(\u001b[0m\u001b[32mi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a retriever \u001b[0m\n",
              "\u001b[32mpη\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m with parameters η that returns \u001b[0m\u001b[32m(\u001b[0m\u001b[32mtop-K truncated\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\ndistributions over text passages given a query x and \u001b[0m\u001b[32m(\u001b[0m\u001b[32mii\u001b[0m\u001b[32m)\u001b[0m\n",
              "\u001b[32ma generator pθ\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, z, y1:i−1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m parametrized\\n\\n\\ndocuments when producing an answer. Concretely, the top K \u001b[0m\n",
              "\u001b[32mdocuments are retrieved using the\\nretriever, and then the generator produces a distribution for the next output \u001b[0m\n",
              "\u001b[32mtoken for each document,\\nbefore marginalizing, and repeating the process with the following output token, \u001b[0m\n",
              "\u001b[32mFormally, we deﬁne:\\npRAG-Token\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m ≈\\nN\\nY\\ni\\nX\\nz∈top-k\u001b[0m\u001b[32m(\u001b[0m\u001b[32mp\u001b[0m\u001b[32m(\u001b[0m\u001b[32m·|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\npη\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32mpθ\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, z, y1:i−1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nFinally, we note \u001b[0m\n",
              "\u001b[32mthat RAG can be used for sequence classiﬁcation tasks by considering the target class\\nas a target sequence of \u001b[0m\n",
              "\u001b[32mlength one, in which case RAG-Sequence and RAG-Token are equivalent.\\n2.2\\nRetriever: DPR\\nThe retrieval component \u001b[0m\n",
              "\u001b[32mpη\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is based on DPR \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m. DPR follows a bi-encoder architecture:\\npη\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m ∝exp\\n\\x00d\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz\u001b[0m\u001b[32m)\u001b[0m\u001b[32m⊤q\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\x01\\nd\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz\u001b[0m\u001b[32m)\u001b[0m\u001b[32m = \u001b[0m\n",
              "\u001b[32mBERTd\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, q\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m = BERTq\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nwhere d\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz\u001b[0m\u001b[32m)\u001b[0m\u001b[32m is a dense representation of a document produced by a BERTBASE document \u001b[0m\n",
              "\u001b[32mencoder \u001b[0m\u001b[32m[\u001b[0m\u001b[32m8\u001b[0m\u001b[32m]\u001b[0m\u001b[32m,\\nand q\u001b[0m\u001b[32m(\u001b[0m\u001b[32mx\u001b[0m\u001b[32m)\u001b[0m\u001b[32m a query representation produced by a query encoder, also based on BERTBASE. \u001b[0m\n",
              "\u001b[32mCalculating\\n\\n\\n1Code to run experiments with RAG has been open-sourced as part of the HuggingFace Transform-\\ners\u001b[0m\n",
              "\u001b[32mLibrary \u001b[0m\u001b[32m[\u001b[0m\u001b[32m66\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and can be found at https://github.com/huggingface/transformers/blob/master/\\nexamples/rag/. An \u001b[0m\n",
              "\u001b[32minteractive demo of RAG models can be found at https://huggingface.co/rag/\\n2\\nby θ that generates a current token \u001b[0m\n",
              "\u001b[32mbased on a context of the previous i −1 tokens y1:i−1, the original\\ninput x and a retrieved passage z.\\nTo train \u001b[0m\n",
              "\u001b[32mthe retriever and generator end-to-end, we treat the retrieved document as a latent variable.\\nWe propose two \u001b[0m\n",
              "\u001b[32mmodels that marginalize over the latent documents in different ways to produce a\\ndistribution over generated text.\u001b[0m\n",
              "\u001b[32mIn one approach, RAG-Sequence, the model uses the same document\\nto predict each target token. The second approach,\u001b[0m\n",
              "\u001b[32mRAG-Token, can predict each target token based\\non a different document. In the following, we formally introduce \u001b[0m\n",
              "\u001b[32mboth models and then describe the\\npη and pθ components, as well as the training and decoding \u001b[0m\n",
              "\u001b[32mprocedure.\\n2.1\\nModels\\n\\n\\nbeams for the marginals. We refer to this decoding procedure as “Thorough Decoding.” \u001b[0m\n",
              "\u001b[32mFor longer\\noutput sequences, |Y | can become large, requiring many forward passes. For more efﬁcient decoding,\\nwe\u001b[0m\n",
              "\u001b[32mcan make a further approximation that pθ\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x, zi\u001b[0m\u001b[32m)\u001b[0m\u001b[32m ≈0 where y was not generated during beam\\nsearch from x, zi. This\u001b[0m\n",
              "\u001b[32mavoids the need to run additional forward passes once the candidate set Y has\\nbeen generated. We refer to this \u001b[0m\n",
              "\u001b[32mdecoding procedure as “Fast Decoding.”\\n3\\nExperiments\\nWe experiment with RAG in a wide range of \u001b[0m\n",
              "\u001b[32mknowledge-intensive tasks. For all experiments, we use\\na single Wikipedia dump for our non-parametric knowledge \u001b[0m\n",
              "\u001b[32msource. Following Lee et al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m31\u001b[0m\u001b[32m]\u001b[0m\u001b[32m and\\nKarpukhin et al. \u001b[0m\u001b[32m[\u001b[0m\u001b[32m26\u001b[0m\u001b[32m]\u001b[0m\u001b[32m, we use the December 2018 dump. Each Wikipedia article\u001b[0m\n",
              "\u001b[32mis split into disjoint\\n100-word chunks, to make a total of 21M documents. We use the document encoder to compute \u001b[0m\n",
              "\u001b[32man\\nembedding for each document, and build a single MIPS index using FAISS \u001b[0m\u001b[32m[\u001b[0m\u001b[32m23\u001b[0m\u001b[32m]\u001b[0m\u001b[32m with a Hierarchical\\n\\n\\npη and pθ \u001b[0m\n",
              "\u001b[32mcomponents, as well as the training and decoding procedure.\\n2.1\\nModels\\nRAG-Sequence Model\\nThe RAG-Sequence \u001b[0m\n",
              "\u001b[32mmodel uses the same retrieved document to generate\\nthe complete sequence. Technically, it treats the retrieved \u001b[0m\n",
              "\u001b[32mdocument as a single latent variable that\\nis marginalized to get the seq2seq probability p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m via a top-K \u001b[0m\n",
              "\u001b[32mapproximation. Concretely, the\\ntop K documents are retrieved using the retriever, and the generator produces the \u001b[0m\n",
              "\u001b[32moutput sequence\\nprobability for each document, which are then marginalized,\\npRAG-Sequence\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\n",
              "\u001b[32m≈\\nX\\nz∈top-k\u001b[0m\u001b[32m(\u001b[0m\u001b[32mp\u001b[0m\u001b[32m(\u001b[0m\u001b[32m·|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\npη\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32mpθ\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x, z\u001b[0m\u001b[32m)\u001b[0m\u001b[32m =\\nX\\nz∈top-k\u001b[0m\u001b[32m(\u001b[0m\u001b[32mp\u001b[0m\u001b[32m(\u001b[0m\u001b[32m·|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\npη\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nN\\nY\\ni\\npθ\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, z, y1:i−1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\nRAG-Token \u001b[0m\n",
              "\u001b[32mModel\\nIn the RAG-Token model we can draw a different latent document for each\\ntarget token and marginalize \u001b[0m\n",
              "\u001b[32maccordingly. This allows the generator to choose content from several\\ndocuments when producing an answer. \u001b[0m\n",
              "\u001b[32mConcretely, the top K documents are retrieved using the\\n\\n\\n    \\n    INSTRUCTIONS:\\n    1. Only answer using \u001b[0m\n",
              "\u001b[32minformation from the provided context and your understanding of AI concepts\\n    2. If you don't know or the answer\u001b[0m\n",
              "\u001b[32misn't in the context, say so honestly\\n    3. Keep responses conversational but technically precise\\n    4. Do not \u001b[0m\n",
              "\u001b[32mmake up information that isn't in the papers\\n    5. Include relevant quotes where helpful, citing the paper\\n    \u001b[0m\n",
              "\u001b[32m\"\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'Tell me about RAG!'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
              "\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RAG, or Retrieval-Augmented Generation, is a method that combines non-parametric and parametric memory to improve sequence-to-sequence tasks. The non-parametric memory is a dense vector index of Wikipedia, and a pre-trained neural retriever is used to access this information. The parametric memory is a pre-trained seq2seq transformer. These two components are combined in a probabilistic model trained end-to-end.\n",
            "\n",
            "There are two variants of RAG: RAG-Sequence and RAG-Token. RAG-Sequence treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. RAG-Token, on the other hand, allows the generator to choose content from several documents when producing an answer by drawing a different latent document for each target token and marginalizing accordingly.\n",
            "\n",
            "The RAG models use a retriever pη(z|x) with parameters η that returns distributions over text passages given a query x and a generator pθ(yi|x, z, y1:i−1) parametrized by θ to produce the next output token. The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next output token for each document, before marginalizing.\n",
            "\n",
            "Compared to retrieve-and-edit style approaches, RAG techniques place less emphasis on lightly editing a retrieved item and more on aggregating content from several pieces of retrieved content. RAG also learns latent retrieval and retrieves evidence documents rather than related training pairs.\n",
            "\n",
            "RAG has been shown to outperform previous extractive approaches for extractive tasks and to generate responses that are more factual, specific, and diverse in knowledge-intensive generation tasks. It also achieved results within 4.3% of state-of-the-art pipeline models for FEVER fact verification and demonstrated that the non-parametric memory can be replaced to update the models’ knowledge as the world changes.\n",
            "\n",
            "Source:\n",
            "\n",
            "> We build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a probabilistic model trained end-to-end (Fig. 1). The retriever provides latent documents conditioned on the input, and the seq2seq model then conditions on these latent documents together with the input to generate the output. We marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same document is responsible for all tokens) or a per-token basis (where different documents are responsible for different tokens).\n",
            "\n",
            "> RAG-Sequence treats the retrieved document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved using the retriever, and the generator produces the output sequence probability for each document, which are then marginalized.\n",
            "\n",
            "> In the RAG-Token model we can draw a different latent document for each target token and marginalize accordingly. This allows the generator to choose content from several documents when producing an answer.\n",
            "\n",
            "> RAG does have some similarities with retrieve-and-edit style approaches, but it does not emphasize lightly editing a retrieved item. Instead, RAG focuses on aggregating content from several pieces of retrieved content.\n",
            "\n",
            "> RAG has been shown to outperform previous extractive approaches for extractive tasks and to generate responses that are more factual, specific, and diverse in knowledge-intensive generation tasks.\n",
            "\n",
            "(The RAG paper, sections 1 and 2)"
          ]
        }
      ],
      "source": [
        "### CODE BLOCK 9 - RAG Chat Interface\n",
        "\n",
        "# Set up models\n",
        "# ChatNVIDIA.get_available_models()  # Uncomment to see available models\n",
        "embedder = NVIDIAEmbeddings(model=\"nvidia/nv-embed-v1\", truncate=\"END\")\n",
        "instruct_llm = ChatNVIDIA(model=\"mistralai/mixtral-8x7b-instruct-v0.1\")\n",
        "# Alternative model option: instruct_llm = ChatNVIDIA(model=\"meta/llama-3.1-8b-instruct\")\n",
        "\n",
        "# Set up conversation memory store\n",
        "print(\"Creating conversation memory store...\")\n",
        "convstore = default_FAISS(embedder)\n",
        "\n",
        "def save_memory_and_get_output(interaction_dict, vector_store):\n",
        "    \"\"\"\n",
        "    Saves both the user question and agent response to memory as embedded texts.\n",
        "    \n",
        "    Args:\n",
        "        interaction_dict: Dictionary containing 'input' (user message) and 'output' (agent response)\n",
        "        vector_store: FAISS vector store for storing conversation history\n",
        "        \n",
        "    Returns:\n",
        "        str: The agent's response (output value from the dictionary)\n",
        "    \"\"\"\n",
        "    from datetime import datetime  # Added missing import\n",
        "    \n",
        "    user_text = f\"User previously responded with {interaction_dict['input']}\"\n",
        "    agent_text = f\"Agent previously responded with {interaction_dict['output']}\"\n",
        "    \n",
        "    # Add both texts to the vector store with appropriate metadata\n",
        "    vector_store.add_texts(\n",
        "        [user_text, agent_text],\n",
        "        metadatas=[\n",
        "            {\"role\": \"user\", \"timestamp\": str(datetime.now())},\n",
        "            {\"role\": \"agent\", \"timestamp\": str(datetime.now())}\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    return interaction_dict[\"output\"]\n",
        "\n",
        "# Create welcome message\n",
        "initial_msg = f\"\"\"\n",
        "# Welcome to Huang's Hype Factory Document Assistant!\n",
        "\n",
        "I'm here to help you with information about the following documents:\n",
        "\n",
        "{doc_string}\n",
        "\n",
        "How can I help you understand these papers today?\n",
        "\"\"\"\n",
        "\n",
        "# Create chat prompt template\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "    You are a knowledgeable research assistant who helps users understand technical AI papers. \n",
        "    \n",
        "    The user's question is: {input}\n",
        "    \n",
        "    CONVERSATION HISTORY:\n",
        "    {history}\n",
        "    \n",
        "    DOCUMENT CONTEXT:\n",
        "    {context}\n",
        "    \n",
        "    INSTRUCTIONS:\n",
        "    1. Only answer using information from the provided context and your understanding of AI concepts\n",
        "    2. If you don't know or the answer isn't in the context, say so honestly\n",
        "    3. Keep responses conversational but technically precise\n",
        "    4. Do not make up information that isn't in the papers\n",
        "    5. Include relevant quotes where helpful, citing the paper\n",
        "    \"\"\"),\n",
        "    (\"user\", \"{input}\")\n",
        "])\n",
        "\n",
        "# Create streaming response chain\n",
        "stream_chain = (\n",
        "    chat_prompt \n",
        "    | RPrint(\"Prompt Sent to LLM\") \n",
        "    | instruct_llm \n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Fix: Create retrieval chain as a proper RunnableLambda\n",
        "def create_context(message):\n",
        "    \"\"\"Function that creates context for the chat prompt.\"\"\"\n",
        "    # First create the base input\n",
        "    context = {\"input\": message}\n",
        "    \n",
        "    # Add conversation history\n",
        "    history_docs = convstore.similarity_search(message, k=5)\n",
        "    history_docs = long_reorder.invoke(history_docs)\n",
        "    history_str = docs2str(history_docs, title=\"Conversation History\")\n",
        "    context[\"history\"] = history_str\n",
        "    \n",
        "    # Add document context\n",
        "    doc_docs = docstore.similarity_search(message, k=8)\n",
        "    doc_docs = long_reorder.invoke(doc_docs)\n",
        "    doc_str = docs2str(doc_docs, title=\"Document Context\")\n",
        "    context[\"context\"] = doc_str\n",
        "    \n",
        "    # Debug print\n",
        "    pprint(f\"Retrieved Context for: {message}\")\n",
        "    \n",
        "    return context\n",
        "\n",
        "retrieval_chain = RunnableLambda(create_context)\n",
        "\n",
        "# Chat generator function\n",
        "def chat_gen(message, history=[], return_buffer=True):\n",
        "    \"\"\"\n",
        "    Generate a streaming chat response using RAG.\n",
        "    \n",
        "    Args:\n",
        "        message: User's input message\n",
        "        history: Chat history (not actively used as we have vector store)\n",
        "        return_buffer: If True, return accumulated buffer; if False, return individual tokens\n",
        "        \n",
        "    Returns:\n",
        "        Generator yielding either the growing buffer or individual tokens\n",
        "    \"\"\"\n",
        "    buffer = \"\"\n",
        "    \n",
        "    # Perform retrieval to get context\n",
        "    context = retrieval_chain.invoke(message)\n",
        "    \n",
        "    # Stream responses from the LLM\n",
        "    for token in stream_chain.stream(context):\n",
        "        buffer += token\n",
        "        if return_buffer:\n",
        "            yield buffer\n",
        "        else:\n",
        "            yield token\n",
        "    \n",
        "    # Save the conversation to memory\n",
        "    save_memory_and_get_output({\n",
        "        \"input\": message,\n",
        "        \"output\": buffer\n",
        "    }, convstore)\n",
        "\n",
        "# Test the implementation\n",
        "test_question = \"Tell me about RAG!\"\n",
        "print(\"\\nTesting chat implementation with query:\", test_question)\n",
        "print(\"\\nResponse:\")\n",
        "\n",
        "for token in chat_gen(test_question, return_buffer=False):\n",
        "    print(token, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kyyeFx1mFnqN",
        "outputId": "2680959c-cdc8-4dee-c079-63e41d4e43b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\dbenn\\AppData\\Local\\Temp\\ipykernel_50348\\1029931177.py:1: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
            "C:\\Users\\dbenn\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\gradio\\chat_interface.py:321: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "* Running on local URL:  http://127.0.0.1:7860\n",
            "* Running on public URL: https://2cb647ab31ff8a9f58.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://2cb647ab31ff8a9f58.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Retrieved Context for: What is BERT?</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mRetrieved Context for: What is BERT?\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">Prompt Sent to LLM: </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">messages</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=[</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">SystemMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">\"\\n    You are a knowledgeable research assistant who helps </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">users understand technical AI papers. \\n    \\n    The user's question is: What is BERT?\\n    \\n    CONVERSATION </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">HISTORY:\\n    --- Conversation History ---\\n\\n\\nUser previously responded with Tell me about RAG!\\n\\n\\nAgent </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">previously responded with RAG, or Retrieval-Augmented Generation, is a method that combines non-parametric and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">parametric memory to improve sequence-to-sequence tasks. The non-parametric memory is a dense vector index of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Wikipedia, and a pre-trained neural retriever is used to access this information. The parametric memory is a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-trained seq2seq transformer. These two components are combined in a probabilistic model trained </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">end-to-end.\\n\\nThere are two variants of RAG: RAG-Sequence and RAG-Token. RAG-Sequence treats the retrieved </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">document as a single latent variable that is marginalized to get the seq2seq probability p(y|x) via a top-K </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">approximation. RAG-Token, on the other hand, allows the generator to choose content from several documents when </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">producing an answer by drawing a different latent document for each target token and marginalizing </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">accordingly.\\n\\nThe RAG models use a retriever pη(z|x) with parameters η that returns distributions over text </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">passages given a query x and a generator pθ(yi|x, z, y1:i−1) parametrized by θ to produce the next output token. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">The top K documents are retrieved using the retriever, and then the generator produces a distribution for the next </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">output token for each document, before marginalizing.\\n\\nCompared to retrieve-and-edit style approaches, RAG </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">techniques place less emphasis on lightly editing a retrieved item and more on aggregating content from several </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pieces of retrieved content. RAG also learns latent retrieval and retrieves evidence documents rather than related </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">training pairs.\\n\\nRAG has been shown to outperform previous extractive approaches for extractive tasks and to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">generate responses that are more factual, specific, and diverse in knowledge-intensive generation tasks. It also </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">achieved results within 4.3% of state-of-the-art pipeline models for FEVER fact verification and demonstrated that </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the non-parametric memory can be replaced to update the models’ knowledge as the world changes.\\n\\nSource:\\n\\n&gt; We </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">build RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">probabilistic model trained end-to-end (Fig. 1). The retriever provides latent documents conditioned on the input, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and the seq2seq model then conditions on these latent documents together with the input to generate the output. We </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">marginalize the latent documents with a top-K approximation, either on a per-output basis (assuming the same </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">document is responsible for all tokens) or a per-token basis (where different documents are responsible for </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">different tokens).\\n\\n&gt; RAG-Sequence treats the retrieved document as a single latent variable that is marginalized</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to get the seq2seq probability p(y|x) via a top-K approximation. Concretely, the top K documents are retrieved </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">using the retriever, and the generator produces the output sequence probability for each document, which are then </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">marginalized.\\n\\n&gt; In the RAG-Token model we can draw a different latent document for each target token and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">marginalize accordingly. This allows the generator to choose content from several documents when producing an </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">answer.\\n\\n&gt; RAG does have some similarities with retrieve-and-edit style approaches, but it does not emphasize </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">lightly editing a retrieved item. Instead, RAG focuses on aggregating content from several pieces of retrieved </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">content.\\n\\n&gt; RAG has been shown to outperform previous extractive approaches for extractive tasks and to generate </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">responses that are more factual, specific, and diverse in knowledge-intensive generation tasks.\\n\\n(The RAG paper, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">sections 1 and 2)\\n\\n\\n    \\n    DOCUMENT CONTEXT:\\n    --- Document Context ---\\n\\n\\nBERT is conceptually simple </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">processing\\ntasks, including pushing the GLUE score to\\n80.5% (7.7% point absolute improvement),\\nMultiNLI accuracy</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">to 86.7% (4.6% absolute\\nimprovement), SQuAD v1.1 question answer-\\ning Test F1 to 93.2 (1.5 point absolute </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">im-\\nprovement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).\\n1\\nIntroduction\\nLanguage model </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks (Dai and Le, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018). These include sentence-level tasks </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">such as\\nnatural language inference (Bowman et al., 2015;\\nWilliams et al., 2018) and paraphrasing (Dolan\\nand </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Brockett, 2005), which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">well as token-level tasks such as\\nnamed entity recognition and question answering,\\n\\n\\ngeNet (Deng et al., 2009; </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Yosinski et al., 2014).\\n3\\nBERT\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">two steps in our\\nframework: pre-training and ﬁne-tuning.\\nDur-\\ning pre-training, the model is trained on </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">unlabeled\\ndata over different pre-training tasks.\\nFor ﬁne-\\ntuning, the BERT model is ﬁrst initialized with\\nthe </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-trained parameters, and all of the param-\\neters are ﬁne-tuned using labeled data from the\\ndownstream tasks. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Each downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">section.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks. There is mini-\\nmal </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">difference between the pre-trained architec-\\nture and the ﬁnal downstream architecture.\\nModel </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Architecture\\nBERT’s model architec-\\nture is a multi-layer bidirectional Transformer en-\\n\\n\\nexpressive </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-trained representations even when\\ndownstream task data is very small.\\n5.3\\nFeature-based Approach with </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">BERT\\nAll of the BERT results presented so far have used\\nthe ﬁne-tuning approach, where a simple classiﬁ-\\ncation </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">layer is added to the pre-trained model, and\\nall parameters are jointly ﬁne-tuned on a down-\\nstream task. </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">However, the feature-based approach,\\nwhere ﬁxed features are extracted from the pre-\\ntrained model, has certain </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">advantages. First, not\\nall tasks can be easily represented by a Trans-\\nformer encoder architecture, and therefore</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">require\\na task-speciﬁc model architecture to be added.\\nSecond, there are major computational beneﬁts\\nto </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">pre-compute an expensive representation of the\\ntraining data once and then run many experiments\\nwith cheaper </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">models on top of this representation.\\nIn this section, we compare the two approaches\\nby applying BERT to the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">CoNLL-2003 Named\\nEntity Recognition (NER) task (Tjong Kim Sang\\nand De Meulder, 2003). In the input to BERT, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">we\\n\\n\\nplay\\n##ing\\n[SEP]\\nmy\\ndog\\nis\\ncute\\n[SEP]\\nInput\\nE[CLS]\\nEhe\\nElikes\\nEplay\\nE##ing\\nE[SEP]\\nEmy\\nEdog\\</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">nEis\\nEcute\\nE[SEP]\\nToken\\nEmbeddings\\nEA\\nEB\\nEB\\nEB\\nEB\\nEB\\nEA\\nEA\\nEA\\nEA\\nEA\\nSegment\\nEmbeddings\\nE0\\nE6\\nE7</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\\nE8\\nE9\\nE10\\nE1\\nE2\\nE3\\nE4\\nE5\\nPosition\\nEmbeddings\\nFigure 2: BERT input representation. The input embeddings </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">are the sum of the token embeddings, the segmenta-\\ntion embeddings and the position embeddings.\\nThe NSP task is </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">closely related to representation-\\nlearning objectives used in Jernite et al. (2017) and\\nLogeswaran and Lee </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">(2018). However, in prior\\nwork, only sentence embeddings are transferred to\\ndown-stream tasks, where BERT </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">transfers all pa-\\nrameters to initialize end-task model parameters.\\nPre-training data The pre-training </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">procedure\\nlargely follows the existing literature on language\\nmodel pre-training. For the pre-training corpus </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">we\\nuse the BooksCorpus (800M words) (Zhu et al.,\\n2015) and English Wikipedia (2,500M words).\\nFor Wikipedia we </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">extract only the text passages\\n\\n\\ngeneral language representations.\\nWe argue that current techniques restrict </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">the\\npower of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches.\\nThe ma-\\njor </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">limitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">that can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">where every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer (Vaswani </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">et al., 2017). Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">applying ﬁne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">incor-\\nporate context from both directions.\\nIn this paper, we improve the ﬁne-tuning based\\napproaches by </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">proposing BERT: Bidirectional\\nEncoder\\nRepresentations\\nfrom\\nTransformers.\\nBERT alleviates the previously </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">mentioned unidi-\\n\\n\\nModel Architecture\\nBERT’s model architec-\\nture is a multi-layer bidirectional Transformer </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">en-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. (2017) and released in\\nthe </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">tensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">identical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">refer readers to\\nVaswani et al. (2017) as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">work, we denote the number of layers\\n(i.e., Transformer blocks) as L, the hidden size as\\nH, and the number of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">self-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE (L=12, H=768, A=12, Total </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Param-\\neters=110M) and BERTLARGE (L=24, H=1024,\\nA=16, Total Parameters=340M).\\nBERTBASE was chosen to have the </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">same model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">uses\\n\\n\\npre-training for language representations. Un-\\nlike Radford et al. (2018), which uses unidirec-\\ntional </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">language models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">representations. This\\nis also in contrast to Peters et al. (2018a), which\\nuses a shallow concatenation of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">independently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">need for many heavily-engineered task-\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\ntuning based representation </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">model that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">outper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks.\\nThe</span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">code and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n2\\nRelated Work\\nThere </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">is a long history of pre-training general lan-\\nguage representations, and we brieﬂy review the\\n\\n\\nBERT: </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Pre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin\\nMing-Wei Chang\\nKenton </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Lee\\nKristina Toutanova\\nGoogle AI Language\\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\\nAbstract\\nWe </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">introduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Representations from\\nTransformers. Unlike recent language repre-\\nsentation models (Peters et al., 2018a; </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">Rad-\\nford et al., 2018), BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">by jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">can be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">tasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">modiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\n\\n\\n   </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">\\n    INSTRUCTIONS:\\n    1. Only answer using information from the provided context and your understanding of AI </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">concepts\\n    2. If you don't know or the answer isn't in the context, say so honestly\\n    3. Keep responses </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">conversational but technically precise\\n    4. Do not make up information that isn't in the papers\\n    5. Include </span>\n",
              "<span style=\"color: #008000; text-decoration-color: #008000\">relevant quotes where helpful, citing the paper\\n    \"</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}), </span>\n",
              "<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">HumanMessage</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">content</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">=</span><span style=\"color: #008000; text-decoration-color: #008000\">'What is BERT?'</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">additional_kwargs</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={}, </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">response_metadata</span><span style=\"color: #76b900; text-decoration-color: #76b900; font-weight: bold\">={})]</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1;38;2;118;185;0mPrompt Sent to LLM: \u001b[0m\u001b[1;33mmessages\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m[\u001b[0m\u001b[1;35mSystemMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m\"\\n    You are a knowledgeable research assistant who helps \u001b[0m\n",
              "\u001b[32musers understand technical AI papers. \\n    \\n    The user's question is: What is BERT?\\n    \\n    CONVERSATION \u001b[0m\n",
              "\u001b[32mHISTORY:\\n    --- Conversation History ---\\n\\n\\nUser previously responded with Tell me about RAG!\\n\\n\\nAgent \u001b[0m\n",
              "\u001b[32mpreviously responded with RAG, or Retrieval-Augmented Generation, is a method that combines non-parametric and \u001b[0m\n",
              "\u001b[32mparametric memory to improve sequence-to-sequence tasks. The non-parametric memory is a dense vector index of \u001b[0m\n",
              "\u001b[32mWikipedia, and a pre-trained neural retriever is used to access this information. The parametric memory is a \u001b[0m\n",
              "\u001b[32mpre-trained seq2seq transformer. These two components are combined in a probabilistic model trained \u001b[0m\n",
              "\u001b[32mend-to-end.\\n\\nThere are two variants of RAG: RAG-Sequence and RAG-Token. RAG-Sequence treats the retrieved \u001b[0m\n",
              "\u001b[32mdocument as a single latent variable that is marginalized to get the seq2seq probability p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m via a top-K \u001b[0m\n",
              "\u001b[32mapproximation. RAG-Token, on the other hand, allows the generator to choose content from several documents when \u001b[0m\n",
              "\u001b[32mproducing an answer by drawing a different latent document for each target token and marginalizing \u001b[0m\n",
              "\u001b[32maccordingly.\\n\\nThe RAG models use a retriever pη\u001b[0m\u001b[32m(\u001b[0m\u001b[32mz|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m with parameters η that returns distributions over text \u001b[0m\n",
              "\u001b[32mpassages given a query x and a generator pθ\u001b[0m\u001b[32m(\u001b[0m\u001b[32myi|x, z, y1:i−1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m parametrized by θ to produce the next output token. \u001b[0m\n",
              "\u001b[32mThe top K documents are retrieved using the retriever, and then the generator produces a distribution for the next \u001b[0m\n",
              "\u001b[32moutput token for each document, before marginalizing.\\n\\nCompared to retrieve-and-edit style approaches, RAG \u001b[0m\n",
              "\u001b[32mtechniques place less emphasis on lightly editing a retrieved item and more on aggregating content from several \u001b[0m\n",
              "\u001b[32mpieces of retrieved content. RAG also learns latent retrieval and retrieves evidence documents rather than related \u001b[0m\n",
              "\u001b[32mtraining pairs.\\n\\nRAG has been shown to outperform previous extractive approaches for extractive tasks and to \u001b[0m\n",
              "\u001b[32mgenerate responses that are more factual, specific, and diverse in knowledge-intensive generation tasks. It also \u001b[0m\n",
              "\u001b[32machieved results within 4.3% of state-of-the-art pipeline models for FEVER fact verification and demonstrated that \u001b[0m\n",
              "\u001b[32mthe non-parametric memory can be replaced to update the models’ knowledge as the world changes.\\n\\nSource:\\n\\n> We \u001b[0m\n",
              "\u001b[32mbuild RAG models where the parametric memory is a pre-trained seq2seq transformer, and the non-parametric memory is\u001b[0m\n",
              "\u001b[32ma dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We combine these components in a \u001b[0m\n",
              "\u001b[32mprobabilistic model trained end-to-end \u001b[0m\u001b[32m(\u001b[0m\u001b[32mFig. 1\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. The retriever provides latent documents conditioned on the input, \u001b[0m\n",
              "\u001b[32mand the seq2seq model then conditions on these latent documents together with the input to generate the output. We \u001b[0m\n",
              "\u001b[32mmarginalize the latent documents with a top-K approximation, either on a per-output basis \u001b[0m\u001b[32m(\u001b[0m\u001b[32massuming the same \u001b[0m\n",
              "\u001b[32mdocument is responsible for all tokens\u001b[0m\u001b[32m)\u001b[0m\u001b[32m or a per-token basis \u001b[0m\u001b[32m(\u001b[0m\u001b[32mwhere different documents are responsible for \u001b[0m\n",
              "\u001b[32mdifferent tokens\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n\\n> RAG-Sequence treats the retrieved document as a single latent variable that is marginalized\u001b[0m\n",
              "\u001b[32mto get the seq2seq probability p\u001b[0m\u001b[32m(\u001b[0m\u001b[32my|x\u001b[0m\u001b[32m)\u001b[0m\u001b[32m via a top-K approximation. Concretely, the top K documents are retrieved \u001b[0m\n",
              "\u001b[32musing the retriever, and the generator produces the output sequence probability for each document, which are then \u001b[0m\n",
              "\u001b[32mmarginalized.\\n\\n> In the RAG-Token model we can draw a different latent document for each target token and \u001b[0m\n",
              "\u001b[32mmarginalize accordingly. This allows the generator to choose content from several documents when producing an \u001b[0m\n",
              "\u001b[32manswer.\\n\\n> RAG does have some similarities with retrieve-and-edit style approaches, but it does not emphasize \u001b[0m\n",
              "\u001b[32mlightly editing a retrieved item. Instead, RAG focuses on aggregating content from several pieces of retrieved \u001b[0m\n",
              "\u001b[32mcontent.\\n\\n> RAG has been shown to outperform previous extractive approaches for extractive tasks and to generate \u001b[0m\n",
              "\u001b[32mresponses that are more factual, specific, and diverse in knowledge-intensive generation tasks.\\n\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mThe RAG paper, \u001b[0m\n",
              "\u001b[32msections 1 and 2\u001b[0m\u001b[32m)\u001b[0m\u001b[32m\\n\\n\\n    \\n    DOCUMENT CONTEXT:\\n    --- Document Context ---\\n\\n\\nBERT is conceptually simple \u001b[0m\n",
              "\u001b[32mand empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\nsults on eleven natural language \u001b[0m\n",
              "\u001b[32mprocessing\\ntasks, including pushing the GLUE score to\\n80.5% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m7.7% point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m,\\nMultiNLI accuracy\u001b[0m\n",
              "\u001b[32mto 86.7% \u001b[0m\u001b[32m(\u001b[0m\u001b[32m4.6% absolute\\nimprovement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, SQuAD v1.1 question answer-\\ning Test F1 to 93.2 \u001b[0m\u001b[32m(\u001b[0m\u001b[32m1.5 point absolute \u001b[0m\n",
              "\u001b[32mim-\\nprovement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and SQuAD v2.0 Test F1 to 83.1\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32m5.1 point absolute improvement\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n1\\nIntroduction\\nLanguage model \u001b[0m\n",
              "\u001b[32mpre-training has been shown to\\nbe effective for improving many natural language\\nprocessing tasks \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDai and Le, \u001b[0m\n",
              "\u001b[32m2015; Peters et al.,\\n2018a; Radford et al., 2018; Howard and Ruder,\\n2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. These include sentence-level tasks \u001b[0m\n",
              "\u001b[32msuch as\\nnatural language inference \u001b[0m\u001b[32m(\u001b[0m\u001b[32mBowman et al., 2015;\\nWilliams et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and paraphrasing \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDolan\\nand \u001b[0m\n",
              "\u001b[32mBrockett, 2005\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which aim to predict the re-\\nlationships between sentences by analyzing them\\nholistically, as \u001b[0m\n",
              "\u001b[32mwell as token-level tasks such as\\nnamed entity recognition and question answering,\\n\\n\\ngeNet \u001b[0m\u001b[32m(\u001b[0m\u001b[32mDeng et al., 2009; \u001b[0m\n",
              "\u001b[32mYosinski et al., 2014\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\n3\\nBERT\\nWe introduce BERT and its detailed implementa-\\ntion in this section. There are \u001b[0m\n",
              "\u001b[32mtwo steps in our\\nframework: pre-training and ﬁne-tuning.\\nDur-\\ning pre-training, the model is trained on \u001b[0m\n",
              "\u001b[32munlabeled\\ndata over different pre-training tasks.\\nFor ﬁne-\\ntuning, the BERT model is ﬁrst initialized with\\nthe \u001b[0m\n",
              "\u001b[32mpre-trained parameters, and all of the param-\\neters are ﬁne-tuned using labeled data from the\\ndownstream tasks. \u001b[0m\n",
              "\u001b[32mEach downstream task has sep-\\narate ﬁne-tuned models, even though they are ini-\\ntialized with the same \u001b[0m\n",
              "\u001b[32mpre-trained parameters. The\\nquestion-answering example in Figure 1 will serve\\nas a running example for this \u001b[0m\n",
              "\u001b[32msection.\\nA distinctive feature of BERT is its uniﬁed ar-\\nchitecture across different tasks. There is mini-\\nmal \u001b[0m\n",
              "\u001b[32mdifference between the pre-trained architec-\\nture and the ﬁnal downstream architecture.\\nModel \u001b[0m\n",
              "\u001b[32mArchitecture\\nBERT’s model architec-\\nture is a multi-layer bidirectional Transformer en-\\n\\n\\nexpressive \u001b[0m\n",
              "\u001b[32mpre-trained representations even when\\ndownstream task data is very small.\\n5.3\\nFeature-based Approach with \u001b[0m\n",
              "\u001b[32mBERT\\nAll of the BERT results presented so far have used\\nthe ﬁne-tuning approach, where a simple classiﬁ-\\ncation \u001b[0m\n",
              "\u001b[32mlayer is added to the pre-trained model, and\\nall parameters are jointly ﬁne-tuned on a down-\\nstream task. \u001b[0m\n",
              "\u001b[32mHowever, the feature-based approach,\\nwhere ﬁxed features are extracted from the pre-\\ntrained model, has certain \u001b[0m\n",
              "\u001b[32madvantages. First, not\\nall tasks can be easily represented by a Trans-\\nformer encoder architecture, and therefore\u001b[0m\n",
              "\u001b[32mrequire\\na task-speciﬁc model architecture to be added.\\nSecond, there are major computational beneﬁts\\nto \u001b[0m\n",
              "\u001b[32mpre-compute an expensive representation of the\\ntraining data once and then run many experiments\\nwith cheaper \u001b[0m\n",
              "\u001b[32mmodels on top of this representation.\\nIn this section, we compare the two approaches\\nby applying BERT to the \u001b[0m\n",
              "\u001b[32mCoNLL-2003 Named\\nEntity Recognition \u001b[0m\u001b[32m(\u001b[0m\u001b[32mNER\u001b[0m\u001b[32m)\u001b[0m\u001b[32m task \u001b[0m\u001b[32m(\u001b[0m\u001b[32mTjong Kim Sang\\nand De Meulder, 2003\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. In the input to BERT, \u001b[0m\n",
              "\u001b[32mwe\\n\\n\\nplay\\n##ing\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\nmy\\ndog\\nis\\ncute\\n\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\nInput\\nE\u001b[0m\u001b[32m[\u001b[0m\u001b[32mCLS\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\nEhe\\nElikes\\nEplay\\nE##ing\\nE\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\nEmy\\nEdog\\\u001b[0m\n",
              "\u001b[32mnEis\\nEcute\\nE\u001b[0m\u001b[32m[\u001b[0m\u001b[32mSEP\u001b[0m\u001b[32m]\u001b[0m\u001b[32m\\nToken\\nEmbeddings\\nEA\\nEB\\nEB\\nEB\\nEB\\nEB\\nEA\\nEA\\nEA\\nEA\\nEA\\nSegment\\nEmbeddings\\nE0\\nE6\\nE7\u001b[0m\n",
              "\u001b[32m\\nE8\\nE9\\nE10\\nE1\\nE2\\nE3\\nE4\\nE5\\nPosition\\nEmbeddings\\nFigure 2: BERT input representation. The input embeddings \u001b[0m\n",
              "\u001b[32mare the sum of the token embeddings, the segmenta-\\ntion embeddings and the position embeddings.\\nThe NSP task is \u001b[0m\n",
              "\u001b[32mclosely related to representation-\\nlearning objectives used in Jernite et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and\\nLogeswaran and Lee \u001b[0m\n",
              "\u001b[32m(\u001b[0m\u001b[32m2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. However, in prior\\nwork, only sentence embeddings are transferred to\\ndown-stream tasks, where BERT \u001b[0m\n",
              "\u001b[32mtransfers all pa-\\nrameters to initialize end-task model parameters.\\nPre-training data The pre-training \u001b[0m\n",
              "\u001b[32mprocedure\\nlargely follows the existing literature on language\\nmodel pre-training. For the pre-training corpus \u001b[0m\n",
              "\u001b[32mwe\\nuse the BooksCorpus \u001b[0m\u001b[32m(\u001b[0m\u001b[32m800M words\u001b[0m\u001b[32m)\u001b[0m\u001b[32m \u001b[0m\u001b[32m(\u001b[0m\u001b[32mZhu et al.,\\n2015\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and English Wikipedia \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2,500M words\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nFor Wikipedia we \u001b[0m\n",
              "\u001b[32mextract only the text passages\\n\\n\\ngeneral language representations.\\nWe argue that current techniques restrict \u001b[0m\n",
              "\u001b[32mthe\\npower of the pre-trained representations, espe-\\ncially for the ﬁne-tuning approaches.\\nThe ma-\\njor \u001b[0m\n",
              "\u001b[32mlimitation is that standard language models are\\nunidirectional, and this limits the choice of archi-\\ntectures \u001b[0m\n",
              "\u001b[32mthat can be used during pre-training. For\\nexample, in OpenAI GPT, the authors use a left-to-\\nright architecture, \u001b[0m\n",
              "\u001b[32mwhere every token can only at-\\ntend to previous tokens in the self-attention layers\\nof the Transformer \u001b[0m\u001b[32m(\u001b[0m\u001b[32mVaswani \u001b[0m\n",
              "\u001b[32met al., 2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m. Such re-\\nstrictions are sub-optimal for sentence-level tasks,\\nand could be very harmful when \u001b[0m\n",
              "\u001b[32mapplying ﬁne-\\ntuning based approaches to token-level tasks such\\nas question answering, where it is crucial to \u001b[0m\n",
              "\u001b[32mincor-\\nporate context from both directions.\\nIn this paper, we improve the ﬁne-tuning based\\napproaches by \u001b[0m\n",
              "\u001b[32mproposing BERT: Bidirectional\\nEncoder\\nRepresentations\\nfrom\\nTransformers.\\nBERT alleviates the previously \u001b[0m\n",
              "\u001b[32mmentioned unidi-\\n\\n\\nModel Architecture\\nBERT’s model architec-\\nture is a multi-layer bidirectional Transformer \u001b[0m\n",
              "\u001b[32men-\\ncoder based on the original implementation de-\\nscribed in Vaswani et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and released in\\nthe \u001b[0m\n",
              "\u001b[32mtensor2tensor library.1 Because the use\\nof Transformers has become common and our im-\\nplementation is almost \u001b[0m\n",
              "\u001b[32midentical to the original,\\nwe will omit an exhaustive background descrip-\\ntion of the model architecture and \u001b[0m\n",
              "\u001b[32mrefer readers to\\nVaswani et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2017\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as well as excellent guides\\nsuch as “The Annotated Transformer.”2\\nIn this\u001b[0m\n",
              "\u001b[32mwork, we denote the number of layers\\n\u001b[0m\u001b[32m(\u001b[0m\u001b[32mi.e., Transformer blocks\u001b[0m\u001b[32m)\u001b[0m\u001b[32m as L, the hidden size as\\nH, and the number of \u001b[0m\n",
              "\u001b[32mself-attention heads as A.3\\nWe primarily report results on two model sizes:\\nBERTBASE \u001b[0m\u001b[32m(\u001b[0m\u001b[32mL\u001b[0m\u001b[32m=\u001b[0m\u001b[32m12\u001b[0m\u001b[32m, \u001b[0m\u001b[32mH\u001b[0m\u001b[32m=\u001b[0m\u001b[32m768\u001b[0m\u001b[32m, \u001b[0m\u001b[32mA\u001b[0m\u001b[32m=\u001b[0m\u001b[32m12\u001b[0m\u001b[32m, Total \u001b[0m\n",
              "\u001b[32mParam-\\\u001b[0m\u001b[32mneters\u001b[0m\u001b[32m=\u001b[0m\u001b[32m110M\u001b[0m\u001b[32m)\u001b[0m\u001b[32m and BERTLARGE \u001b[0m\u001b[32m(\u001b[0m\u001b[32mL\u001b[0m\u001b[32m=\u001b[0m\u001b[32m24\u001b[0m\u001b[32m, \u001b[0m\u001b[32mH\u001b[0m\u001b[32m=\u001b[0m\u001b[32m1024\u001b[0m\u001b[32m,\\\u001b[0m\u001b[32mnA\u001b[0m\u001b[32m=\u001b[0m\u001b[32m16\u001b[0m\u001b[32m, Total \u001b[0m\u001b[32mParameters\u001b[0m\u001b[32m=\u001b[0m\u001b[32m340M\u001b[0m\u001b[32m)\u001b[0m\u001b[32m.\\nBERTBASE was chosen to have the \u001b[0m\n",
              "\u001b[32msame model\\nsize as OpenAI GPT for comparison purposes.\\nCritically, however, the BERT Transformer \u001b[0m\n",
              "\u001b[32muses\\n\\n\\npre-training for language representations. Un-\\nlike Radford et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which uses unidirec-\\ntional \u001b[0m\n",
              "\u001b[32mlanguage models for pre-training, BERT\\nuses masked language models to enable pre-\\ntrained deep bidirectional \u001b[0m\n",
              "\u001b[32mrepresentations. This\\nis also in contrast to Peters et al. \u001b[0m\u001b[32m(\u001b[0m\u001b[32m2018a\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, which\\nuses a shallow concatenation of \u001b[0m\n",
              "\u001b[32mindependently\\ntrained left-to-right and right-to-left LMs.\\n• We show that pre-trained representations reduce\\nthe\u001b[0m\n",
              "\u001b[32mneed for many heavily-engineered task-\\nspeciﬁc architectures. BERT is the ﬁrst ﬁne-\\ntuning based representation \u001b[0m\n",
              "\u001b[32mmodel that achieves\\nstate-of-the-art performance on a large suite\\nof sentence-level and token-level tasks, \u001b[0m\n",
              "\u001b[32moutper-\\nforming many task-speciﬁc architectures.\\n• BERT advances the state of the art for eleven\\nNLP tasks.\\nThe\u001b[0m\n",
              "\u001b[32mcode and pre-trained mod-\\nels are available at https://github.com/\\ngoogle-research/bert.\\n2\\nRelated Work\\nThere \u001b[0m\n",
              "\u001b[32mis a long history of pre-training general lan-\\nguage representations, and we brieﬂy review the\\n\\n\\nBERT: \u001b[0m\n",
              "\u001b[32mPre-training of Deep Bidirectional Transformers for\\nLanguage Understanding\\nJacob Devlin\\nMing-Wei Chang\\nKenton \u001b[0m\n",
              "\u001b[32mLee\\nKristina Toutanova\\nGoogle AI Language\\n\u001b[0m\u001b[32m{\u001b[0m\u001b[32mjacobdevlin,mingweichang,kentonl,kristout\u001b[0m\u001b[32m}\u001b[0m\u001b[32m@google.com\\nAbstract\\nWe \u001b[0m\n",
              "\u001b[32mintroduce a new language representa-\\ntion model called BERT, which stands for\\nBidirectional Encoder \u001b[0m\n",
              "\u001b[32mRepresentations from\\nTransformers. Unlike recent language repre-\\nsentation models \u001b[0m\u001b[32m(\u001b[0m\u001b[32mPeters et al., 2018a; \u001b[0m\n",
              "\u001b[32mRad-\\nford et al., 2018\u001b[0m\u001b[32m)\u001b[0m\u001b[32m, BERT is designed to pre-\\ntrain deep bidirectional representations from\\nunlabeled text \u001b[0m\n",
              "\u001b[32mby jointly conditioning on both\\nleft and right context in all layers. As a re-\\nsult, the pre-trained BERT model \u001b[0m\n",
              "\u001b[32mcan be ﬁne-\\ntuned with just one additional output layer\\nto create state-of-the-art models for a wide\\nrange of \u001b[0m\n",
              "\u001b[32mtasks, such as question answering and\\nlanguage inference, without substantial task-\\nspeciﬁc architecture \u001b[0m\n",
              "\u001b[32mmodiﬁcations.\\nBERT is conceptually simple and empirically\\npowerful.\\nIt obtains new state-of-the-art re-\\n\\n\\n   \u001b[0m\n",
              "\u001b[32m\\n    INSTRUCTIONS:\\n    1. Only answer using information from the provided context and your understanding of AI \u001b[0m\n",
              "\u001b[32mconcepts\\n    2. If you don't know or the answer isn't in the context, say so honestly\\n    3. Keep responses \u001b[0m\n",
              "\u001b[32mconversational but technically precise\\n    4. Do not make up information that isn't in the papers\\n    5. Include \u001b[0m\n",
              "\u001b[32mrelevant quotes where helpful, citing the paper\\n    \"\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\n",
              "\u001b[1;35mHumanMessage\u001b[0m\u001b[1;38;2;118;185;0m(\u001b[0m\u001b[1;33mcontent\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[32m'What is BERT?'\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33madditional_kwargs\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m, \u001b[0m\u001b[1;33mresponse_metadata\u001b[0m\u001b[1;38;2;118;185;0m=\u001b[0m\u001b[1;38;2;118;185;0m{\u001b[0m\u001b[1;38;2;118;185;0m}\u001b[0m\u001b[1;38;2;118;185;0m)\u001b[0m\u001b[1;38;2;118;185;0m]\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://2cb647ab31ff8a9f58.gradio.live\n",
            "Closing server running on port: 7860\n"
          ]
        }
      ],
      "source": [
        "chatbot = gr.Chatbot(value = [[None, initial_msg]])\n",
        "demo = gr.ChatInterface(chat_gen, chatbot=chatbot).queue()\n",
        "\n",
        "try:\n",
        "    demo.launch(debug=True, share=True, show_api=False)\n",
        "    demo.close()\n",
        "except Exception as e:\n",
        "    demo.close()\n",
        "    print(e)\n",
        "    raise e"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
